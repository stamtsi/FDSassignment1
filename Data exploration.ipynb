{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "import json\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import metrics\n",
    "\n",
    "stopword = nltk.corpus.stopwords.words('english')\n",
    "ps = nltk.PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if heavier emoji stripper is needed\n",
    "\n",
    "# def remove_emoji(string):\n",
    "#     emoji_pattern = re.compile(\"[\"\n",
    "#                                u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "#                                u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "#                                u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "#                                u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "#                                u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "#                                u\"\\U00002702-\\U000027B0\"\n",
    "#                                u\"\\U00002702-\\U000027B0\"\n",
    "#                                u\"\\U000024C2-\\U0001F251\"\n",
    "#                                u\"\\U0001f926-\\U0001f937\"\n",
    "#                                u\"\\U00010000-\\U0010ffff\"\n",
    "#                                u\"\\u2640-\\u2642\"\n",
    "#                                u\"\\u2600-\\u2B55\"\n",
    "#                                u\"\\u200d\"\n",
    "#                                u\"\\u23cf\"\n",
    "#                                u\"\\u23e9\"\n",
    "#                                u\"\\u231a\"\n",
    "#                                u\"\\ufe0f\"  # dingbats\n",
    "#                                u\"\\u3030\"\n",
    "#                                \"]+\", flags=re.UNICODE)\n",
    "#     return emoji_pattern.sub(r'', string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punct(text):\n",
    "    text  = \"\".join([char for char in text if char not in string.punctuation])\n",
    "    text = re.sub('[0-9]+', '', text)\n",
    "    return text\n",
    "\n",
    "def tokenization(text):\n",
    "    text = re.split('\\W+', text)\n",
    "    return text\n",
    "\n",
    "def remove_urls(text):\n",
    "    text = re.sub(r'https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()@:%_\\+.~#?&//=]*)', '', text)\n",
    "    return text\n",
    "\n",
    "def remove_mentions(text):\n",
    "    text = re.sub(r'(^|[^@\\w])@(\\w{1,15})\\b', '', text)\n",
    "    return text\n",
    "\n",
    "def remove_empty_tokens(text):\n",
    "    text = list(filter(None, text))\n",
    "    return text\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    text = [word for word in text if word not in stopword]\n",
    "    return text\n",
    "\n",
    "def stemming(text):\n",
    "    text = [ps.stem(word) for word in text]\n",
    "    return text\n",
    "\n",
    "def deEmojify(text):\n",
    "    regrex_pattern = re.compile(pattern = \"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           \"]+\", flags = re.UNICODE)\n",
    "    return regrex_pattern.sub(r'',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target          id                          date      flag  \\\n",
       "0       0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
       "1       0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "2       0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "3       0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4       0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "\n",
       "              user                                               text  \n",
       "0  _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
       "1    scotthamilton  is upset that he can't update his Facebook by ...  \n",
       "2         mattycus  @Kenichan I dived many times for the ball. Man...  \n",
       "3          ElleCTF    my whole body feels itchy and like its on fire   \n",
       "4           Karoli  @nationwideclass no, it's not behaving at all....  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 0 = negative, 4 = positive\n",
    "\n",
    "df = pd.read_csv('data/training.1600000.processed.noemoticon.csv', encoding='ISO-8859-1', header=None, names=['target', 'id', 'date', 'flag', 'user', 'text'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "      <th>no_urls</th>\n",
       "      <th>unmentioned</th>\n",
       "      <th>depunctualized</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>improved_tokenized</th>\n",
       "      <th>nonstop</th>\n",
       "      <th>rejoined</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "      <td>@switchfoot  - Awww, that's a bummer.  You sho...</td>\n",
       "      <td>- Awww, that's a bummer.  You shoulda got Da...</td>\n",
       "      <td>Awww thats a bummer  You shoulda got David ...</td>\n",
       "      <td>[, awww, thats, a, bummer, you, shoulda, got, ...</td>\n",
       "      <td>[awww, thats, a, bummer, you, shoulda, got, da...</td>\n",
       "      <td>[awww, thats, bummer, shoulda, got, david, car...</td>\n",
       "      <td>awww thats bummer shoulda got david carr third...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "      <td>is upset that he cant update his Facebook by t...</td>\n",
       "      <td>[is, upset, that, he, cant, update, his, faceb...</td>\n",
       "      <td>[is, upset, that, he, cant, update, his, faceb...</td>\n",
       "      <td>[upset, cant, update, facebook, texting, might...</td>\n",
       "      <td>upset cant update facebook texting might cry r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "      <td>I dived many times for the ball. Managed to s...</td>\n",
       "      <td>I dived many times for the ball Managed to sa...</td>\n",
       "      <td>[, i, dived, many, times, for, the, ball, mana...</td>\n",
       "      <td>[i, dived, many, times, for, the, ball, manage...</td>\n",
       "      <td>[dived, many, times, ball, managed, save, rest...</td>\n",
       "      <td>dived many times ball managed save rest go bounds</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>[my, whole, body, feels, itchy, and, like, its...</td>\n",
       "      <td>[my, whole, body, feels, itchy, and, like, its...</td>\n",
       "      <td>[whole, body, feels, itchy, like, fire]</td>\n",
       "      <td>whole body feels itchy like fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "      <td>no, it's not behaving at all. i'm mad. why am...</td>\n",
       "      <td>no its not behaving at all im mad why am i he...</td>\n",
       "      <td>[, no, its, not, behaving, at, all, im, mad, w...</td>\n",
       "      <td>[no, its, not, behaving, at, all, im, mad, why...</td>\n",
       "      <td>[behaving, im, mad, cant, see]</td>\n",
       "      <td>behaving im mad cant see</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target          id                          date      flag  \\\n",
       "0       0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
       "1       0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "2       0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "3       0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4       0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "\n",
       "              user                                               text  \\\n",
       "0  _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...   \n",
       "1    scotthamilton  is upset that he can't update his Facebook by ...   \n",
       "2         mattycus  @Kenichan I dived many times for the ball. Man...   \n",
       "3          ElleCTF    my whole body feels itchy and like its on fire    \n",
       "4           Karoli  @nationwideclass no, it's not behaving at all....   \n",
       "\n",
       "                                             no_urls  \\\n",
       "0  @switchfoot  - Awww, that's a bummer.  You sho...   \n",
       "1  is upset that he can't update his Facebook by ...   \n",
       "2  @Kenichan I dived many times for the ball. Man...   \n",
       "3    my whole body feels itchy and like its on fire    \n",
       "4  @nationwideclass no, it's not behaving at all....   \n",
       "\n",
       "                                         unmentioned  \\\n",
       "0    - Awww, that's a bummer.  You shoulda got Da...   \n",
       "1  is upset that he can't update his Facebook by ...   \n",
       "2   I dived many times for the ball. Managed to s...   \n",
       "3    my whole body feels itchy and like its on fire    \n",
       "4   no, it's not behaving at all. i'm mad. why am...   \n",
       "\n",
       "                                      depunctualized  \\\n",
       "0     Awww thats a bummer  You shoulda got David ...   \n",
       "1  is upset that he cant update his Facebook by t...   \n",
       "2   I dived many times for the ball Managed to sa...   \n",
       "3    my whole body feels itchy and like its on fire    \n",
       "4   no its not behaving at all im mad why am i he...   \n",
       "\n",
       "                                           tokenized  \\\n",
       "0  [, awww, thats, a, bummer, you, shoulda, got, ...   \n",
       "1  [is, upset, that, he, cant, update, his, faceb...   \n",
       "2  [, i, dived, many, times, for, the, ball, mana...   \n",
       "3  [my, whole, body, feels, itchy, and, like, its...   \n",
       "4  [, no, its, not, behaving, at, all, im, mad, w...   \n",
       "\n",
       "                                  improved_tokenized  \\\n",
       "0  [awww, thats, a, bummer, you, shoulda, got, da...   \n",
       "1  [is, upset, that, he, cant, update, his, faceb...   \n",
       "2  [i, dived, many, times, for, the, ball, manage...   \n",
       "3  [my, whole, body, feels, itchy, and, like, its...   \n",
       "4  [no, its, not, behaving, at, all, im, mad, why...   \n",
       "\n",
       "                                             nonstop  \\\n",
       "0  [awww, thats, bummer, shoulda, got, david, car...   \n",
       "1  [upset, cant, update, facebook, texting, might...   \n",
       "2  [dived, many, times, ball, managed, save, rest...   \n",
       "3            [whole, body, feels, itchy, like, fire]   \n",
       "4                     [behaving, im, mad, cant, see]   \n",
       "\n",
       "                                            rejoined  \n",
       "0  awww thats bummer shoulda got david carr third...  \n",
       "1  upset cant update facebook texting might cry r...  \n",
       "2  dived many times ball managed save rest go bounds  \n",
       "3                   whole body feels itchy like fire  \n",
       "4                           behaving im mad cant see  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['no_urls'] = df['text'].apply(lambda x: remove_urls(x)) #remove urls\n",
    "df['unmentioned'] = df['no_urls'].apply(lambda x: remove_mentions(x)) #remove mentions\n",
    "df['depunctualized'] = df['unmentioned'].apply(lambda x: remove_punct(x)) #remove punctuations\n",
    "df['tokenized'] = df['depunctualized'].apply(lambda x: tokenization(x.lower())) #tokenize\n",
    "df['improved_tokenized'] = df['tokenized'].apply(lambda x: remove_empty_tokens(x)) #remove empty tokens\n",
    "df['nonstop'] = df['improved_tokenized'].apply(lambda x: remove_stopwords(x)) #remove stopwords\n",
    "#df['stemmed'] = df['nonstop'].apply(lambda x: stemming(x)) #stem the tokens\n",
    "df['rejoined'] = df['nonstop'].apply(lambda x: \" \".join(x)) #rejoin for vectorization\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract to numpy arrays\n",
    "\n",
    "y = df.target.to_numpy()\n",
    "X = df.rejoined.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [ 693767 1081409  678396 ...   87567 1393165   31307] TEST: [1442636   44280  702174 ...  813844 1180632 1077844]\n"
     ]
    }
   ],
   "source": [
    "#split into train (80%) and test (20%)\n",
    "\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, train_size=0.8)\n",
    "\n",
    "for train_index, test_index in sss.split(X, y):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_t = X[train_index], X[test_index]\n",
    "    y_train, y_t = y[train_index], y[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST: [293622 252902 281338 ... 263641 233725 292555] Val: [294995 131589 262548 ... 114874 102871 184032]\n"
     ]
    }
   ],
   "source": [
    "#split test into test (10%) and val (10%)\n",
    "\n",
    "sss_val = StratifiedShuffleSplit(n_splits=1, test_size=0.5, train_size=0.5)\n",
    "\n",
    "for test_i, val_i in sss_val.split(X_t, y_t):\n",
    "    print(\"TEST:\", test_i, \"Val:\", val_i)\n",
    "    X_test, X_val = X_t[test_i], X_t[val_i]\n",
    "    y_test, y_val = y_t[test_i], y_t[val_i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1280000,) (1280000,) (160000,) (160000,) (160000,) (160000,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape, X_val.shape, y_val.shape) #check if split sizes are correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vectorize the tweets\n",
    "\n",
    "# cv = CountVectorizer(binary=True)\n",
    "# cv.fit(X_train) #fit the vectorizer\n",
    "\n",
    "#X_train_vectorized = cv.transform(X_train)\n",
    "X_test_vectorized = cv.transform(X_test)\n",
    "X_val_vectorized = cv.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 160000 points : 37394\n"
     ]
    }
   ],
   "source": [
    "y_pred = MultinomialNB().fit(X_train_vectorized, y_train).predict(X_val_vectorized)\n",
    "print(\"Number of mislabeled points out of a total %d points : %d\"\n",
    "       % (X_val_vectorized.shape[0], (y_val != y_pred).sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2673125\n"
     ]
    }
   ],
   "source": [
    "# mnb = MultinomialNB().fit(X_train_vectorized, y_train) #final trained model\n",
    "# mnb_predictions = mnb.predict(X_val_vectorized)\n",
    "mnb_accuracy = mnb.score(X_test_vectorized, y_test)\n",
    "print(mnb_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in y_test:\n",
    "    if i == 1:\n",
    "        print('what')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb_predictions = mnb.predict(X_test_vectorized)\n",
    "transformed_mnb_predictions = [0 if prediction == True else 4 for prediction in mnb_predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.397     0.535     0.455     80000\n",
      "           1      0.000     0.000     0.000         0\n",
      "           4      0.000     0.000     0.000     80000\n",
      "\n",
      "   micro avg      0.267     0.267     0.267    160000\n",
      "   macro avg      0.132     0.178     0.152    160000\n",
      "weighted avg      0.198     0.267     0.228    160000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jyrom\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\jyrom\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_test, mnb_predictions, digits=3, target_names=['negative sentiment', 'positive sentiment']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save trained model as pickle\n",
    "\n",
    "# filename = 'MNB_sentiment_classifier_model_lemmatized.sav'\n",
    "# with open(filename, 'wb') as f_out:\n",
    "#     pickle.dump((mnb, cv), f_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.74414375\n"
     ]
    }
   ],
   "source": [
    "#stemmed model\n",
    "\n",
    "#load the model from disk\n",
    "filename = 'models/MNB_sentiment_classifier_model_lemmatized.sav'\n",
    "\n",
    "stemmed_model, stemmed_cv = pickle.load(open(filename, 'rb'))\n",
    "#result = loaded_model.score(X_test, Y_test)\n",
    "#print(result)\n",
    "\n",
    "X_test_vectorized = stemmed_cv.transform(X_test) #transform with pretrained vectorizer\n",
    "X_val_vectorized = stemmed_cv.transform(X_val) #transform with pretrained vectorizer\n",
    "\n",
    "stemmed_accuracy = stemmed_model.score(X_val_vectorized, y_val) #calculate accuracy\n",
    "print(stemmed_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    precision    recall  f1-score   support\n",
      "\n",
      "negative sentiment      0.743     0.749     0.746     80000\n",
      "positive sentiment      0.747     0.741     0.744     80000\n",
      "\n",
      "         micro avg      0.745     0.745     0.745    160000\n",
      "         macro avg      0.745     0.745     0.745    160000\n",
      "      weighted avg      0.745     0.745     0.745    160000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stemmed_predictions = stemmed_model.predict(X_test_vectorized) #let the model predict on unseen test set\n",
    "print(metrics.classification_report(y_test, stemmed_predictions, digits=3, target_names=['negative sentiment', 'positive sentiment']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.78853125\n"
     ]
    }
   ],
   "source": [
    "#unstemmed model\n",
    "\n",
    "#load the model from disk\n",
    "filename = 'models/MNB_sentiment_classifier_model.sav'\n",
    "\n",
    "unstemmed_model, unstemmed_cv = pickle.load(open(filename, 'rb'))\n",
    "#result = loaded_model.score(X_test, Y_test)\n",
    "#print(result)\n",
    "\n",
    "X_test_vectorized = unstemmed_cv.transform(X_test) #transform with pretrained vectorizer\n",
    "X_val_vectorized = unstemmed_cv.transform(X_val) #transform with pretrained vectorizer\n",
    "\n",
    "unstemmed_accuracy = unstemmed_model.score(X_val_vectorized, y_val) calculate accuracy\n",
    "print(unstemmed_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    precision    recall  f1-score   support\n",
      "\n",
      "negative sentiment      0.767     0.832     0.798     80000\n",
      "positive sentiment      0.816     0.748     0.780     80000\n",
      "\n",
      "         micro avg      0.790     0.790     0.790    160000\n",
      "         macro avg      0.792     0.790     0.789    160000\n",
      "      weighted avg      0.792     0.790     0.789    160000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "unstemmed_predictions = unstemmed_model.predict(X_test_vectorized) #let the model predict on unseen test set\n",
    "print(metrics.classification_report(y_test, unstemmed_predictions, digits=3, target_names=['negative sentiment', 'positive sentiment']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the model on provided tweet dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "#take a subset of 30 tweets\n",
    "\n",
    "jsonlist = []\n",
    "c = 0\n",
    "with open('data/geotagged_tweets_20160812-0912.jsons', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        c+=1\n",
    "        jsonlist.append(json.loads(line))\n",
    "        if c == 5000:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': 'NOJUSTICE', 'indices': [62, 72]},\n",
       " {'text': 'TrumpPence', 'indices': [93, 104]}]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jsonlist[1]['entities']['hashtags']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'screen_name': 'theblaze',\n",
       "  'name': 'TheBlaze',\n",
       "  'id': 10774652,\n",
       "  'id_str': '10774652',\n",
       "  'indices': [0, 9]},\n",
       " {'screen_name': 'realDonaldTrump',\n",
       "  'name': 'Donald J. Trump',\n",
       "  'id': 25073877,\n",
       "  'id_str': '25073877',\n",
       "  'indices': [10, 26]}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jsonlist[3]['entities']['user_mentions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input = [tweet['text'] for tweet in jsonlist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>no_urls</th>\n",
       "      <th>unmentioned</th>\n",
       "      <th>depunctualized</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>improved_tokenized</th>\n",
       "      <th>nonstop</th>\n",
       "      <th>stemmed</th>\n",
       "      <th>rejoined</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@theblaze @realDonaldTrump https://t.co/TY9DlZ...</td>\n",
       "      <td>@theblaze @realDonaldTrump</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[, ]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@BarackObama \\n@FBI\\n@LORETTALYNCH \\nALL IN CO...</td>\n",
       "      <td>@BarackObama \\n@FBI\\n@LORETTALYNCH \\nALL IN CO...</td>\n",
       "      <td>\\nALL IN COLLUSION TOGETHER \\n\\n#NOJUSTICE \\...</td>\n",
       "      <td>\\nALL IN COLLUSION TOGETHER \\n\\nNOJUSTICE \\n...</td>\n",
       "      <td>[, all, in, collusion, together, nojustice, tr...</td>\n",
       "      <td>[all, in, collusion, together, nojustice, trum...</td>\n",
       "      <td>[collusion, together, nojustice, trumppence]</td>\n",
       "      <td>[collus, togeth, nojustic, trumppenc]</td>\n",
       "      <td>collus togeth nojustic trumppenc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@theblaze @realDonaldTrump https://t.co/n050DB...</td>\n",
       "      <td>@theblaze @realDonaldTrump</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[, ]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@HillaryClinton he will do in one year all the...</td>\n",
       "      <td>@HillaryClinton he will do in one year all the...</td>\n",
       "      <td>he will do in one year all the things you sho...</td>\n",
       "      <td>he will do in one year all the things you sho...</td>\n",
       "      <td>[, he, will, do, in, one, year, all, the, thin...</td>\n",
       "      <td>[he, will, do, in, one, year, all, the, things...</td>\n",
       "      <td>[one, year, things, done, eight]</td>\n",
       "      <td>[one, year, thing, done, eight]</td>\n",
       "      <td>one year thing done eight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#CNN #newday clear #Trump deliberately throwin...</td>\n",
       "      <td>#CNN #newday clear #Trump deliberately throwin...</td>\n",
       "      <td>#CNN #newday clear #Trump deliberately throwin...</td>\n",
       "      <td>CNN newday clear Trump deliberately throwing t...</td>\n",
       "      <td>[cnn, newday, clear, trump, deliberately, thro...</td>\n",
       "      <td>[cnn, newday, clear, trump, deliberately, thro...</td>\n",
       "      <td>[cnn, newday, clear, trump, deliberately, thro...</td>\n",
       "      <td>[cnn, newday, clear, trump, deliber, throw, ra...</td>\n",
       "      <td>cnn newday clear trump deliber throw racein kn...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  @theblaze @realDonaldTrump https://t.co/TY9DlZ...   \n",
       "1  @BarackObama \\n@FBI\\n@LORETTALYNCH \\nALL IN CO...   \n",
       "2  @theblaze @realDonaldTrump https://t.co/n050DB...   \n",
       "3  @HillaryClinton he will do in one year all the...   \n",
       "4  #CNN #newday clear #Trump deliberately throwin...   \n",
       "\n",
       "                                             no_urls  \\\n",
       "0                        @theblaze @realDonaldTrump    \n",
       "1  @BarackObama \\n@FBI\\n@LORETTALYNCH \\nALL IN CO...   \n",
       "2                        @theblaze @realDonaldTrump    \n",
       "3  @HillaryClinton he will do in one year all the...   \n",
       "4  #CNN #newday clear #Trump deliberately throwin...   \n",
       "\n",
       "                                         unmentioned  \\\n",
       "0                                                      \n",
       "1    \\nALL IN COLLUSION TOGETHER \\n\\n#NOJUSTICE \\...   \n",
       "2                                                      \n",
       "3   he will do in one year all the things you sho...   \n",
       "4  #CNN #newday clear #Trump deliberately throwin...   \n",
       "\n",
       "                                      depunctualized  \\\n",
       "0                                                      \n",
       "1    \\nALL IN COLLUSION TOGETHER \\n\\nNOJUSTICE \\n...   \n",
       "2                                                      \n",
       "3   he will do in one year all the things you sho...   \n",
       "4  CNN newday clear Trump deliberately throwing t...   \n",
       "\n",
       "                                           tokenized  \\\n",
       "0                                               [, ]   \n",
       "1  [, all, in, collusion, together, nojustice, tr...   \n",
       "2                                               [, ]   \n",
       "3  [, he, will, do, in, one, year, all, the, thin...   \n",
       "4  [cnn, newday, clear, trump, deliberately, thro...   \n",
       "\n",
       "                                  improved_tokenized  \\\n",
       "0                                                 []   \n",
       "1  [all, in, collusion, together, nojustice, trum...   \n",
       "2                                                 []   \n",
       "3  [he, will, do, in, one, year, all, the, things...   \n",
       "4  [cnn, newday, clear, trump, deliberately, thro...   \n",
       "\n",
       "                                             nonstop  \\\n",
       "0                                                 []   \n",
       "1       [collusion, together, nojustice, trumppence]   \n",
       "2                                                 []   \n",
       "3                   [one, year, things, done, eight]   \n",
       "4  [cnn, newday, clear, trump, deliberately, thro...   \n",
       "\n",
       "                                             stemmed  \\\n",
       "0                                                 []   \n",
       "1              [collus, togeth, nojustic, trumppenc]   \n",
       "2                                                 []   \n",
       "3                    [one, year, thing, done, eight]   \n",
       "4  [cnn, newday, clear, trump, deliber, throw, ra...   \n",
       "\n",
       "                                            rejoined  \n",
       "0                                                     \n",
       "1                   collus togeth nojustic trumppenc  \n",
       "2                                                     \n",
       "3                          one year thing done eight  \n",
       "4  cnn newday clear trump deliber throw racein kn...  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#clean the subset of tweets\n",
    "\n",
    "testframe = pd.DataFrame(test_input, columns=['text'])\n",
    "testframe['no_urls'] = testframe['text'].apply(lambda x: remove_urls(x)) #remove urls\n",
    "testframe['unmentioned'] = testframe['no_urls'].apply(lambda x: remove_mentions(x)) #remove mentions\n",
    "testframe['depunctualized'] = testframe['unmentioned'].apply(lambda x: remove_punct(x)) #remove punctuations\n",
    "testframe['tokenized'] = testframe['depunctualized'].apply(lambda x: tokenization(x.lower())) #tokenize\n",
    "testframe['improved_tokenized'] = testframe['tokenized'].apply(lambda x: remove_empty_tokens(x)) #remove empty tokens\n",
    "testframe['nonstop'] = testframe['improved_tokenized'].apply(lambda x: remove_stopwords(x)) #remove stopwords\n",
    "testframe['stemmed'] = testframe['nonstop'].apply(lambda x: stemming(x))\n",
    "testframe['rejoined'] = testframe['stemmed'].apply(lambda x: \" \".join(x))\n",
    "testframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lorettalynch collus togeth nojustic trumppenc',\n",
       " 'one year thing done eight',\n",
       " 'cnn newday clear trump deliber throw racein knew isi destabil mideast start wiraq invas',\n",
       " 'wouldnt recogn lie came mouth continu nevertrump',\n",
       " 'trump trumppenc makeamericagreatagain',\n",
       " 'kid know su someon that beauti thing human could anoth human',\n",
       " 'cofound isi crook evil lie witch live',\n",
       " 'want comparison tri maim vet pre amp post iraq pullout bar graph',\n",
       " 'total concur elect cra cra n corrupt gov mind blow trump last hope',\n",
       " 'issu idiot claim found isi trump go hell lie amp steal shame',\n",
       " 'cant stand ortak look windont settl teamgov youin',\n",
       " 'isnt rape alleg get attent caus seem probabl',\n",
       " 'stole white hous furnitur',\n",
       " 'gop plead w trump control behavior week want year terrifi nevertrump crazi',\n",
       " 'isi cofound hillari clinton obama also devil hillari sit left hand devil',\n",
       " 'come jesu meet earth suppos',\n",
       " 'hanniti think disbar ignor mr hamburg dishonest',\n",
       " 'stop worri msm lie focu econampimmampdfn ur spprtr alreadi know she scum amp msm tank',\n",
       " 'true us offici sell arm isi',\n",
       " 'wake dem hillari cofound isi',\n",
       " 'oh go channel damn husband',\n",
       " 'morningjo wonder mental member laugh amp clap alleg child rapist',\n",
       " 'dont hate democrat hate evil ideolog abort immigr proudli present good america',\n",
       " 'yr crazi perform due dementia clever plot btwn amp clinton bring gop def con',\n",
       " 'isi polici creat upris youareresponsiblehillari',\n",
       " 'republican pull shenanigan expel parti',\n",
       " 'heystop use mediaif bias dont useprint cabl',\n",
       " 'preach']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testinput = [tweet for tweet in testframe.rejoined.values if tweet] #drop empty tweets\n",
    "testinput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vectorize with the fitted vectorizer\n",
    "\n",
    "test_input_vectorized = cv.transform(testinput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = mnb.predict(test_input_vectorized) #predict the sentiment of the tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>lorettalynch collus togeth nojustic trumppenc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>one year thing done eight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>cnn newday clear trump deliber throw racein kn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>wouldnt recogn lie came mouth continu nevertrump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>trump trumppenc makeamericagreatagain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>kid know su someon that beauti thing human cou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>cofound isi crook evil lie witch live</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>want comparison tri maim vet pre amp post iraq...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>total concur elect cra cra n corrupt gov mind ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>issu idiot claim found isi trump go hell lie a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>cant stand ortak look windont settl teamgov youin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>isnt rape alleg get attent caus seem probabl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>stole white hous furnitur</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>gop plead w trump control behavior week want y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>4</td>\n",
       "      <td>isi cofound hillari clinton obama also devil h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>4</td>\n",
       "      <td>come jesu meet earth suppos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>4</td>\n",
       "      <td>hanniti think disbar ignor mr hamburg dishonest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>stop worri msm lie focu econampimmampdfn ur sp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>true us offici sell arm isi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>4</td>\n",
       "      <td>wake dem hillari cofound isi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0</td>\n",
       "      <td>oh go channel damn husband</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>4</td>\n",
       "      <td>morningjo wonder mental member laugh amp clap ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0</td>\n",
       "      <td>dont hate democrat hate evil ideolog abort imm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>4</td>\n",
       "      <td>yr crazi perform due dementia clever plot btwn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0</td>\n",
       "      <td>isi polici creat upris youareresponsiblehillari</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0</td>\n",
       "      <td>republican pull shenanigan expel parti</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0</td>\n",
       "      <td>heystop use mediaif bias dont useprint cabl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>4</td>\n",
       "      <td>preach</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    sentiment                                              tweet\n",
       "0           4      lorettalynch collus togeth nojustic trumppenc\n",
       "1           0                          one year thing done eight\n",
       "2           0  cnn newday clear trump deliber throw racein kn...\n",
       "3           0   wouldnt recogn lie came mouth continu nevertrump\n",
       "4           4              trump trumppenc makeamericagreatagain\n",
       "5           4  kid know su someon that beauti thing human cou...\n",
       "6           0              cofound isi crook evil lie witch live\n",
       "7           0  want comparison tri maim vet pre amp post iraq...\n",
       "8           0  total concur elect cra cra n corrupt gov mind ...\n",
       "9           0  issu idiot claim found isi trump go hell lie a...\n",
       "10          0  cant stand ortak look windont settl teamgov youin\n",
       "11          0       isnt rape alleg get attent caus seem probabl\n",
       "12          0                          stole white hous furnitur\n",
       "13          0  gop plead w trump control behavior week want y...\n",
       "14          4  isi cofound hillari clinton obama also devil h...\n",
       "15          4                        come jesu meet earth suppos\n",
       "16          4    hanniti think disbar ignor mr hamburg dishonest\n",
       "17          0  stop worri msm lie focu econampimmampdfn ur sp...\n",
       "18          0                        true us offici sell arm isi\n",
       "19          4                       wake dem hillari cofound isi\n",
       "20          0                         oh go channel damn husband\n",
       "21          4  morningjo wonder mental member laugh amp clap ...\n",
       "22          0  dont hate democrat hate evil ideolog abort imm...\n",
       "23          4  yr crazi perform due dementia clever plot btwn...\n",
       "24          0    isi polici creat upris youareresponsiblehillari\n",
       "25          0             republican pull shenanigan expel parti\n",
       "26          0        heystop use mediaif bias dont useprint cabl\n",
       "27          4                                             preach"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sentiment and stemmed tweet\n",
    "\n",
    "output = pd.DataFrame(testinput, test).reset_index()\n",
    "output.columns = ['sentiment', 'tweet']\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@theblaze @realDonaldTrump https://t.co/TY9DlZ584c\n",
      "@BarackObama @FBI@LORETTALYNCH ALL IN COLLUSION TOGETHER #NOJUSTICE @realDonaldTrump #TrumpPence https://t.co/5GMNZq40V3\n",
      "@theblaze @realDonaldTrump https://t.co/n050DBSpv0\n",
      "@HillaryClinton he will do in one year all the things you should have done in eight\n",
      "#CNN #newday clear #Trump deliberately throwing this race,in 2007 he knew that #ISIS and destabilization of Mideast started w/Iraq invasion\n",
      "@realDonaldTrump, you wouldn't recognize a lie if it came from your own mouth, and they do continually. #NeverTrump https://t.co/pKSQM8yikm\n",
      "#Trump2016 #TrumpPence16 #MakeAmericaGreatAgain  https://t.co/l5UsYANVc9\n",
      "\"Kid, you know, suing someone? Thats the most beautiful thing 1 human being could do to another human being\" @funnyordie @realDonaldTrumps\n",
      "@HillaryClinton you ARE the co-founder of ISIS, you crooked, evil, lying, witch. How can you live with yourself?\n",
      "@Geraldanthro @NeilTurner_ @realDonaldTrump want to do a comparison try maimed Vets pre &amp; post Iraq pullout. Bar graph that. @washingtonpost\n",
      "@mike4193496 @realDonaldTrump I TOTALLY CONCUR!! This Election is just CRA CRA n Corruption in our Gov is Mind Blowing!! Trump= Last Hope!!!\n",
      "@realDonaldTrump @elsolarverde What issues? Your idiot claim that she \"founded\" ISIS? Trump will go to Hell for lying &amp; stealing. Shame!\n",
      "Can't stand @HillaryClinton or @realDonaldTrump?Take a look. They can win...don't settle. #15for15 #TeamGov #YouIn  https://t.co/YK336aaH98\n",
      "@CribBoss @WesSmith123 @realDonaldTrump why isn't his rape allegations getting more attention cause it's seeming more probable he did it\n",
      "@HillaryClinton @TeamUSA @realDonaldTrump was this before or after you stole the White House furniture?\n",
      "GOP pleading w Trump \"Just control your behavior for a few weeks, then do what you want for 4 years\" - Terrifying! #NeverTrump  #Crazy\n",
      "@HillaryClinton ISIS co-founder Hillary Clinton. Obama also he is the devil and Hillary sits at the left hand of the devil\n",
      "Come to Jesus meeting!!!! What on earth is that supposed to be?  https://t.co/a3lOpTtFig\n",
      "@sherrilee7 @seanhannity @HillaryClinton #Hannity what do you think of being disbarred? You are ignorant Mr. Hamburg or just dishonest?\n",
      "@realDonaldTrump stop worrying about the MSM lies. Focus on econ&amp;imm&amp;dfns.  Ur spprtrs already know she's scum &amp; MSM is in the tank for her.\n",
      "Is it true? US officially selling arms to ISIS?@RT_Erdogan @econofpak @Asad_Umar @AsimBajwaISPR @HillaryClinton https://t.co/LyimNK4wV4\n",
      "@HFA @HillaryClinton wake up Dems Hillary is cofounder of Isis\n",
      "@HillaryClinton @TeamUSA @realDonaldTrump oh here you go again, channeling that damn husband of yours\n",
      "#morningjoe Have to wonder about the mentality of @NAHBhome members who laughed &amp; clapped at alleged child rapist @realDonaldTrump @JoeNBC\n",
      "I don't hate Democrats. I hate the evil ideology of abortion and immigration they so proudly present as good America https://t.co/eR6GLDhj1M\n",
      "@realDonaldTrump Is yr  crazy performance due to dementia or is this a very clever plot btwn you &amp; Clinton to bring down GOP. Defs a con\n",
      "2008 there was no #ISIS.  It was @BarackObama and @HillaryClinton policies that created the uprising. #YouareResponsibleHillary @FoxNews\n",
      "@LouDobbs @realDonaldTrump @FoxNews These Republicans pulling these shenanigans should be expelled from the party!!!\n",
      "Hey,stop using the media.If they are so biased against you don't use,print or cable. https://t.co/TOtgdmW7wC\n",
      "Preach! https://t.co/0E1MDwU48r\n"
     ]
    }
   ],
   "source": [
    "#pre cleaned tweet\n",
    "\n",
    "for i in testframe.text:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "from gensim import corpora, models\n",
    "from pprint import pprint\n",
    "stemmer = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hashtags</th>\n",
       "      <th>location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>764039733076897792</th>\n",
       "      <td>[NOJUSTICE, TrumpPence]</td>\n",
       "      <td>Baton Rouge, LA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>764039849850482689</th>\n",
       "      <td>[CNN, newday, Trump, ISIS]</td>\n",
       "      <td>Baltimore, MD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>764039917924069376</th>\n",
       "      <td>[NeverTrump]</td>\n",
       "      <td>Palm Springs, CA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>764039925146742784</th>\n",
       "      <td>[Trump2016, TrumpPence16, MakeAmericaGreatAgain]</td>\n",
       "      <td>Hammersmith, London</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>764039994247819264</th>\n",
       "      <td>[15for15, TeamGov, YouIn]</td>\n",
       "      <td>Middletown, KY</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                            hashtags  \\\n",
       "764039733076897792                           [NOJUSTICE, TrumpPence]   \n",
       "764039849850482689                        [CNN, newday, Trump, ISIS]   \n",
       "764039917924069376                                      [NeverTrump]   \n",
       "764039925146742784  [Trump2016, TrumpPence16, MakeAmericaGreatAgain]   \n",
       "764039994247819264                         [15for15, TeamGov, YouIn]   \n",
       "\n",
       "                               location  \n",
       "764039733076897792      Baton Rouge, LA  \n",
       "764039849850482689        Baltimore, MD  \n",
       "764039917924069376     Palm Springs, CA  \n",
       "764039925146742784  Hammersmith, London  \n",
       "764039994247819264       Middletown, KY  "
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#all hashtags and locations\n",
    "hashtags = {}\n",
    "\n",
    "with open('data/geotagged_tweets_20160812-0912.jsons', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        tweet = json.loads(line)\n",
    "        if len(tweet['entities']['hashtags']) > 0:\n",
    "            hashtags[tweet['id']] = {} #make a nested dict for every tweet\n",
    "            hashtaglist = [] #make empty list for hashtags\n",
    "            for item in tweet['entities']['hashtags']: #loop through hashtags in tweet\n",
    "                hashtaglist.append(item['text'])\n",
    "            hashtags[tweet['id']]['hashtags'] = hashtaglist\n",
    "            try:\n",
    "                hashtags[tweet['id']]['location'] = tweet['place']['full_name']\n",
    "            except TypeError:\n",
    "                continue\n",
    "            \n",
    "\n",
    "#clean the subset of tweets\n",
    "hframe = pd.DataFrame.from_dict(hashtags).T\n",
    "hframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "hframe.to_pickle('data/hashtags_locations.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>no_urls</th>\n",
       "      <th>unmentioned</th>\n",
       "      <th>depunctualized</th>\n",
       "      <th>lemmed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@theblaze @realDonaldTrump https://t.co/TY9DlZ...</td>\n",
       "      <td>@theblaze @realDonaldTrump</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@BarackObama \\n@FBI\\n@LORETTALYNCH \\nALL IN CO...</td>\n",
       "      <td>@BarackObama \\n@FBI\\n@LORETTALYNCH \\nALL IN CO...</td>\n",
       "      <td>\\nALL IN COLLUSION TOGETHER \\n\\n#NOJUSTICE \\...</td>\n",
       "      <td>\\nALL IN COLLUSION TOGETHER \\n\\nNOJUSTICE \\n...</td>\n",
       "      <td>[collus, nojustic, trumppenc]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@theblaze @realDonaldTrump https://t.co/n050DB...</td>\n",
       "      <td>@theblaze @realDonaldTrump</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@realDonaldTrump, you wouldn't recognize a lie...</td>\n",
       "      <td>@realDonaldTrump, you wouldn't recognize a lie...</td>\n",
       "      <td>, you wouldn't recognize a lie if it came from...</td>\n",
       "      <td>you wouldnt recognize a lie if it came from y...</td>\n",
       "      <td>[wouldnt, recogn, come, mouth, continu, nevert...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"Kid, you know, suing someone? Thats the most ...</td>\n",
       "      <td>\"Kid, you know, suing someone? Thats the most ...</td>\n",
       "      <td>\"Kid, you know, suing someone? Thats the most ...</td>\n",
       "      <td>Kid you know suing someone Thats the most beau...</td>\n",
       "      <td>[know, sue, that, beauti, thing, human, human]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  @theblaze @realDonaldTrump https://t.co/TY9DlZ...   \n",
       "1  @BarackObama \\n@FBI\\n@LORETTALYNCH \\nALL IN CO...   \n",
       "2  @theblaze @realDonaldTrump https://t.co/n050DB...   \n",
       "3  @realDonaldTrump, you wouldn't recognize a lie...   \n",
       "4  \"Kid, you know, suing someone? Thats the most ...   \n",
       "\n",
       "                                             no_urls  \\\n",
       "0                        @theblaze @realDonaldTrump    \n",
       "1  @BarackObama \\n@FBI\\n@LORETTALYNCH \\nALL IN CO...   \n",
       "2                        @theblaze @realDonaldTrump    \n",
       "3  @realDonaldTrump, you wouldn't recognize a lie...   \n",
       "4  \"Kid, you know, suing someone? Thats the most ...   \n",
       "\n",
       "                                         unmentioned  \\\n",
       "0                                                      \n",
       "1    \\nALL IN COLLUSION TOGETHER \\n\\n#NOJUSTICE \\...   \n",
       "2                                                      \n",
       "3  , you wouldn't recognize a lie if it came from...   \n",
       "4  \"Kid, you know, suing someone? Thats the most ...   \n",
       "\n",
       "                                      depunctualized  \\\n",
       "0                                                      \n",
       "1    \\nALL IN COLLUSION TOGETHER \\n\\nNOJUSTICE \\n...   \n",
       "2                                                      \n",
       "3   you wouldnt recognize a lie if it came from y...   \n",
       "4  Kid you know suing someone Thats the most beau...   \n",
       "\n",
       "                                              lemmed  \n",
       "0                                                 []  \n",
       "1                      [collus, nojustic, trumppenc]  \n",
       "2                                                 []  \n",
       "3  [wouldnt, recogn, come, mouth, continu, nevert...  \n",
       "4     [know, sue, that, beauti, thing, human, human]  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#take the text of all tweets\n",
    "clinton_tweets = []\n",
    "trump_tweets = []\n",
    "\n",
    "#maybe add obama too?\n",
    "\n",
    "with open('data/geotagged_tweets_20160812-0912.jsons', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        tweet = json.loads(line)\n",
    "        mentionlist = []\n",
    "        for mention in tweet['entities']['user_mentions']:\n",
    "            mentionlist.append(mention['id'])\n",
    "        if 1339835893 in mentionlist and 25073877 not in mentionlist:\n",
    "            clinton_tweets.append(tweet['text'])\n",
    "        if 25073877 in mentionlist and 1339835893 not in mentionlist:\n",
    "            trump_tweets.append(tweet['text'])\n",
    "            \n",
    "            \n",
    "\n",
    "#clean the subset of tweets\n",
    "cframe = pd.DataFrame(clinton_tweets, columns=['text'])\n",
    "cframe['no_urls'] = cframe['text'].apply(lambda x: remove_urls(x)) #remove urls\n",
    "cframe['unmentioned'] = cframe['no_urls'].apply(lambda x: remove_mentions(x)) #remove mentions\n",
    "cframe['depunctualized'] = cframe['unmentioned'].apply(lambda x: remove_punct(x)) #remove punctuations\n",
    "cframe['lemmed'] = cframe['depunctualized'].apply(lambda x: preprocess(x)) #lemmatize\n",
    "cframe.head()\n",
    "\n",
    "\n",
    "tframe = pd.DataFrame(trump_tweets, columns=['text'])\n",
    "tframe['no_urls'] = tframe['text'].apply(lambda x: remove_urls(x)) #remove urls\n",
    "tframe['unmentioned'] = tframe['no_urls'].apply(lambda x: remove_mentions(x)) #remove mentions\n",
    "tframe['depunctualized'] = tframe['unmentioned'].apply(lambda x: remove_punct(x)) #remove punctuations\n",
    "tframe['lemmed'] = tframe['depunctualized'].apply(lambda x: preprocess(x)) #lemmatize\n",
    "tframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>no_urls</th>\n",
       "      <th>unmentioned</th>\n",
       "      <th>depunctualized</th>\n",
       "      <th>lemmed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>764039724818272256</th>\n",
       "      <td>Frontenac, MO</td>\n",
       "      <td>@theblaze @realDonaldTrump https://t.co/TY9DlZ...</td>\n",
       "      <td>@theblaze @realDonaldTrump</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>764039733076897792</th>\n",
       "      <td>Baton Rouge, LA</td>\n",
       "      <td>@BarackObama \\n@FBI\\n@LORETTALYNCH \\nALL IN CO...</td>\n",
       "      <td>@BarackObama \\n@FBI\\n@LORETTALYNCH \\nALL IN CO...</td>\n",
       "      <td>\\nALL IN COLLUSION TOGETHER \\n\\n#NOJUSTICE \\...</td>\n",
       "      <td>\\nALL IN COLLUSION TOGETHER \\n\\nNOJUSTICE \\n...</td>\n",
       "      <td>[collus, nojustic, trumppenc]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>764039769244348417</th>\n",
       "      <td>Frontenac, MO</td>\n",
       "      <td>@theblaze @realDonaldTrump https://t.co/n050DB...</td>\n",
       "      <td>@theblaze @realDonaldTrump</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>764039917924069376</th>\n",
       "      <td>Palm Springs, CA</td>\n",
       "      <td>@realDonaldTrump, you wouldn't recognize a lie...</td>\n",
       "      <td>@realDonaldTrump, you wouldn't recognize a lie...</td>\n",
       "      <td>, you wouldn't recognize a lie if it came from...</td>\n",
       "      <td>you wouldnt recognize a lie if it came from y...</td>\n",
       "      <td>[wouldnt, recogn, come, mouth, continu, nevert...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>764039926161604608</th>\n",
       "      <td>Secaucus, NJ</td>\n",
       "      <td>\"Kid, you know, suing someone? Thats the most ...</td>\n",
       "      <td>\"Kid, you know, suing someone? Thats the most ...</td>\n",
       "      <td>\"Kid, you know, suing someone? Thats the most ...</td>\n",
       "      <td>Kid you know suing someone Thats the most beau...</td>\n",
       "      <td>[know, sue, that, beauti, thing, human, human]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            location  \\\n",
       "764039724818272256     Frontenac, MO   \n",
       "764039733076897792   Baton Rouge, LA   \n",
       "764039769244348417     Frontenac, MO   \n",
       "764039917924069376  Palm Springs, CA   \n",
       "764039926161604608      Secaucus, NJ   \n",
       "\n",
       "                                                                 text  \\\n",
       "764039724818272256  @theblaze @realDonaldTrump https://t.co/TY9DlZ...   \n",
       "764039733076897792  @BarackObama \\n@FBI\\n@LORETTALYNCH \\nALL IN CO...   \n",
       "764039769244348417  @theblaze @realDonaldTrump https://t.co/n050DB...   \n",
       "764039917924069376  @realDonaldTrump, you wouldn't recognize a lie...   \n",
       "764039926161604608  \"Kid, you know, suing someone? Thats the most ...   \n",
       "\n",
       "                                                              no_urls  \\\n",
       "764039724818272256                        @theblaze @realDonaldTrump    \n",
       "764039733076897792  @BarackObama \\n@FBI\\n@LORETTALYNCH \\nALL IN CO...   \n",
       "764039769244348417                        @theblaze @realDonaldTrump    \n",
       "764039917924069376  @realDonaldTrump, you wouldn't recognize a lie...   \n",
       "764039926161604608  \"Kid, you know, suing someone? Thats the most ...   \n",
       "\n",
       "                                                          unmentioned  \\\n",
       "764039724818272256                                                      \n",
       "764039733076897792    \\nALL IN COLLUSION TOGETHER \\n\\n#NOJUSTICE \\...   \n",
       "764039769244348417                                                      \n",
       "764039917924069376  , you wouldn't recognize a lie if it came from...   \n",
       "764039926161604608  \"Kid, you know, suing someone? Thats the most ...   \n",
       "\n",
       "                                                       depunctualized  \\\n",
       "764039724818272256                                                      \n",
       "764039733076897792    \\nALL IN COLLUSION TOGETHER \\n\\nNOJUSTICE \\n...   \n",
       "764039769244348417                                                      \n",
       "764039917924069376   you wouldnt recognize a lie if it came from y...   \n",
       "764039926161604608  Kid you know suing someone Thats the most beau...   \n",
       "\n",
       "                                                               lemmed  \n",
       "764039724818272256                                                 []  \n",
       "764039733076897792                      [collus, nojustic, trumppenc]  \n",
       "764039769244348417                                                 []  \n",
       "764039917924069376  [wouldnt, recogn, come, mouth, continu, nevert...  \n",
       "764039926161604608     [know, sue, that, beauti, thing, human, human]  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#with location added\n",
    "\n",
    "#take the text of all tweets\n",
    "clinton_tweets = {}\n",
    "trump_tweets = {}\n",
    "\n",
    "with open('data/geotagged_tweets_20160812-0912.jsons', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        tweet = json.loads(line)\n",
    "        mentionlist = []\n",
    "        for mention in tweet['entities']['user_mentions']:\n",
    "            mentionlist.append(mention['id'])\n",
    "        if 1339835893 in mentionlist and 25073877 not in mentionlist:\n",
    "            clinton_tweets[tweet['id']] = {} \n",
    "            clinton_tweets[tweet['id']]['text'] = tweet['text']\n",
    "            try:\n",
    "                clinton_tweets[tweet['id']]['location'] = tweet['place']['full_name']\n",
    "            except TypeError:\n",
    "                continue\n",
    "        if 25073877 in mentionlist and 1339835893 not in mentionlist:\n",
    "            trump_tweets[tweet['id']] = {} \n",
    "            trump_tweets[tweet['id']]['text'] = tweet['text']\n",
    "            try:\n",
    "                trump_tweets[tweet['id']]['location'] = tweet['place']['full_name']\n",
    "            except TypeError:\n",
    "                continue\n",
    "            \n",
    "\n",
    "#clean the subset of tweets\n",
    "cframe = pd.DataFrame.from_dict(clinton_tweets).T\n",
    "cframe['no_urls'] = cframe['text'].apply(lambda x: remove_urls(x)) #remove urls\n",
    "cframe['unmentioned'] = cframe['no_urls'].apply(lambda x: remove_mentions(x)) #remove mentions\n",
    "cframe['depunctualized'] = cframe['unmentioned'].apply(lambda x: remove_punct(x)) #remove punctuations\n",
    "cframe['lemmed'] = cframe['depunctualized'].apply(lambda x: preprocess(x)) #lemmatize\n",
    "cframe.head()\n",
    "\n",
    "\n",
    "tframe = pd.DataFrame.from_dict(trump_tweets).T\n",
    "tframe['no_urls'] = tframe['text'].apply(lambda x: remove_urls(x)) #remove urls\n",
    "tframe['unmentioned'] = tframe['no_urls'].apply(lambda x: remove_mentions(x)) #remove mentions\n",
    "tframe['depunctualized'] = tframe['unmentioned'].apply(lambda x: remove_punct(x)) #remove punctuations\n",
    "tframe['lemmed'] = tframe['depunctualized'].apply(lambda x: preprocess(x)) #lemmatize\n",
    "tframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('data/candidate_tweets.pkl', 'wb') as f:\n",
    "#     pickle.dump((clinton_tweets, trump_tweets), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'models/MNB_sentiment_classifier_model.sav'\n",
    "\n",
    "mnb, cv = pickle.load(open(filename, 'rb'))\n",
    "#result = loaded_model.score(X_test, Y_test)\n",
    "#print(result)\n",
    "\n",
    "#mnb_accuracy = mnb.score(X_val_vectorized, y_val)\n",
    "#print(mnb_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "cframe['tokenized'] = cframe['depunctualized'].apply(lambda x: tokenization(x.lower())) #tokenize\n",
    "cframe['improved_tokenized'] = cframe['tokenized'].apply(lambda x: remove_empty_tokens(x)) #remove empty tokens\n",
    "cframe['nonstop'] = cframe['improved_tokenized'].apply(lambda x: remove_stopwords(x)) #remove stopwords\n",
    "cframe['nw_input'] = cframe['nonstop'].apply(lambda x: \" \".join(x)) #rejoin for vectorization\n",
    "\n",
    "tframe['tokenized'] = tframe['depunctualized'].apply(lambda x: tokenization(x.lower())) #tokenize\n",
    "tframe['improved_tokenized'] = tframe['tokenized'].apply(lambda x: remove_empty_tokens(x)) #remove empty tokens\n",
    "tframe['nonstop'] = tframe['improved_tokenized'].apply(lambda x: remove_stopwords(x)) #remove stopwords\n",
    "tframe['nw_input'] = tframe['nonstop'].apply(lambda x: \" \".join(x)) #rejoin for vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "clinton = cframe[cframe.nw_input != ''].copy()\n",
    "trump = tframe[tframe.nw_input != ''].copy()\n",
    "\n",
    "clinton_vectorized = cv.transform(clinton.nw_input.to_numpy())\n",
    "trump_vectorized = cv.transform(trump.nw_input.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "clinton_predictions = mnb.predict(clinton_vectorized)\n",
    "trump_predictions = mnb.predict(trump_vectorized)\n",
    "\n",
    "transformed_clinton_predictions = ['Negative' if prediction == 0 else 'Positive' for prediction in clinton_predictions]\n",
    "transformed_trump_predictions = ['Negative' if prediction == 0 else 'Positive' for prediction in trump_predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "clinton['sentiment'] = transformed_clinton_predictions\n",
    "trump['sentiment'] = transformed_trump_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformdict = {'tweet': clinton, 'sentiment': transformed_clinton_predictions}\n",
    "# clinton_sentiment = pd.DataFrame(transformdict)\n",
    "# clinton_sentiment.head()\n",
    "\n",
    "# transformdict = {'tweet': trump, 'sentiment': transformed_trump_predictions}\n",
    "# trump_sentiment = pd.DataFrame(transformdict)\n",
    "# trump_sentiment.head()\n",
    "\n",
    "clinton.to_pickle('clinton_with_location.pkl')\n",
    "trump.to_pickle('trump_with_location.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>one year things done eight</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cofounder isis crooked evil lying witch live</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>isis cofounder hillary clinton obama also devi...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hannity think disbarred ignorant mr hamburg di...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>true us officially selling arms isis</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>wake dems hillary cofounder isis</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>isis policies created uprising youareresponsib...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>lady hillary best president thank much</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>way today always</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>corrupt perjurer treason antun soldout america...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>please stop politicalpandering ur memorializin...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ur piece propaganda doesnt take account dems n...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>trump mentally ill perversion fuck daughter</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>im</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>remember dan qual rodney king murphy brown show</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>trump selling us russia also mental case perve...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>want small business president guess goldman sa...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>foley sex offender</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>read twitter removes dementia</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>trust implicitlyexplicitly senate colleagues e...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                tweet sentiment\n",
       "0                          one year things done eight  Positive\n",
       "1        cofounder isis crooked evil lying witch live  Negative\n",
       "2   isis cofounder hillary clinton obama also devi...  Negative\n",
       "3   hannity think disbarred ignorant mr hamburg di...  Negative\n",
       "4                true us officially selling arms isis  Positive\n",
       "5                    wake dems hillary cofounder isis  Negative\n",
       "6   isis policies created uprising youareresponsib...  Positive\n",
       "7              lady hillary best president thank much  Positive\n",
       "8                                    way today always  Positive\n",
       "9   corrupt perjurer treason antun soldout america...  Negative\n",
       "10  please stop politicalpandering ur memorializin...  Negative\n",
       "11  ur piece propaganda doesnt take account dems n...  Negative\n",
       "12        trump mentally ill perversion fuck daughter  Negative\n",
       "13                                                 im  Positive\n",
       "14    remember dan qual rodney king murphy brown show  Positive\n",
       "15  trump selling us russia also mental case perve...  Negative\n",
       "16  want small business president guess goldman sa...  Positive\n",
       "17                                 foley sex offender  Negative\n",
       "18                      read twitter removes dementia  Positive\n",
       "19  trust implicitlyexplicitly senate colleagues e...  Positive"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clinton_sentiment.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>collusion together nojustice trumppence</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>wouldnt recognize lie came mouth continually n...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kid know suing someone thats beautiful thing h...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>want comparison try maimed vets pre amp post i...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>totally concur election cra cra n corruption g...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>issues idiot claim founded isis trump go hell ...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>isnt rape allegations getting attention cause ...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>stop worrying msm lies focus econampimmampdfns...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>morningjoe wonder mentality members laughed am...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>yr crazy performance due dementia clever plot ...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>republicans pulling shenanigans expelled party</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>open next rally look win advise illegals self ...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>msm trying distract u amp keep balance focus h...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>job opening kremlin cc</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>videotape enough expose fascist racism</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>conmanbaby taking america back make america gr...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>people forget expecially close election</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>trump insult democracy tweet defeat</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>watch hour talk trumppence</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>morningjoe alleged child rapist cant discuss i...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                tweet sentiment\n",
       "0             collusion together nojustice trumppence  Negative\n",
       "1   wouldnt recognize lie came mouth continually n...  Negative\n",
       "2   kid know suing someone thats beautiful thing h...  Positive\n",
       "3   want comparison try maimed vets pre amp post i...  Positive\n",
       "4   totally concur election cra cra n corruption g...  Negative\n",
       "5   issues idiot claim founded isis trump go hell ...  Negative\n",
       "6   isnt rape allegations getting attention cause ...  Negative\n",
       "7   stop worrying msm lies focus econampimmampdfns...  Negative\n",
       "8   morningjoe wonder mentality members laughed am...  Positive\n",
       "9   yr crazy performance due dementia clever plot ...  Negative\n",
       "10     republicans pulling shenanigans expelled party  Positive\n",
       "11  open next rally look win advise illegals self ...  Positive\n",
       "12  msm trying distract u amp keep balance focus h...  Negative\n",
       "13                             job opening kremlin cc  Positive\n",
       "14             videotape enough expose fascist racism  Negative\n",
       "15  conmanbaby taking america back make america gr...  Negative\n",
       "16            people forget expecially close election  Positive\n",
       "17                trump insult democracy tweet defeat  Negative\n",
       "18                         watch hour talk trumppence  Positive\n",
       "19  morningjoe alleged child rapist cant discuss i...  Negative"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trump_sentiment.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clinton_sentiment.to_pickle('data/clinton_sentiment.pkl')\n",
    "# trump_sentiment.to_pickle('data/trump_sentiment.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(102744, 2) (273298, 2)\n"
     ]
    }
   ],
   "source": [
    "clinton_sentiment = pd.read_pickle('data/clinton_sentiment.pkl')\n",
    "trump_sentiment = pd.read_pickle('data/trump_sentiment.pkl')\n",
    "print(clinton_sentiment.shape, trump_sentiment.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>no_urls</th>\n",
       "      <th>unmentioned</th>\n",
       "      <th>depunctualized</th>\n",
       "      <th>lemmed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@theblaze @realDonaldTrump https://t.co/TY9DlZ...</td>\n",
       "      <td>@theblaze @realDonaldTrump</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@BarackObama \\n@FBI\\n@LORETTALYNCH \\nALL IN CO...</td>\n",
       "      <td>@BarackObama \\n@FBI\\n@LORETTALYNCH \\nALL IN CO...</td>\n",
       "      <td>\\nALL IN COLLUSION TOGETHER \\n\\n#NOJUSTICE \\...</td>\n",
       "      <td>\\nALL IN COLLUSION TOGETHER \\n\\nNOJUSTICE \\n...</td>\n",
       "      <td>[collus, nojustic, trumppenc]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@theblaze @realDonaldTrump https://t.co/n050DB...</td>\n",
       "      <td>@theblaze @realDonaldTrump</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@HillaryClinton he will do in one year all the...</td>\n",
       "      <td>@HillaryClinton he will do in one year all the...</td>\n",
       "      <td>he will do in one year all the things you sho...</td>\n",
       "      <td>he will do in one year all the things you sho...</td>\n",
       "      <td>[year, thing]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#CNN #newday clear #Trump deliberately throwin...</td>\n",
       "      <td>#CNN #newday clear #Trump deliberately throwin...</td>\n",
       "      <td>#CNN #newday clear #Trump deliberately throwin...</td>\n",
       "      <td>CNN newday clear Trump deliberately throwing t...</td>\n",
       "      <td>[newday, clear, trump, deliber, throw, racein,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  @theblaze @realDonaldTrump https://t.co/TY9DlZ...   \n",
       "1  @BarackObama \\n@FBI\\n@LORETTALYNCH \\nALL IN CO...   \n",
       "2  @theblaze @realDonaldTrump https://t.co/n050DB...   \n",
       "3  @HillaryClinton he will do in one year all the...   \n",
       "4  #CNN #newday clear #Trump deliberately throwin...   \n",
       "\n",
       "                                             no_urls  \\\n",
       "0                        @theblaze @realDonaldTrump    \n",
       "1  @BarackObama \\n@FBI\\n@LORETTALYNCH \\nALL IN CO...   \n",
       "2                        @theblaze @realDonaldTrump    \n",
       "3  @HillaryClinton he will do in one year all the...   \n",
       "4  #CNN #newday clear #Trump deliberately throwin...   \n",
       "\n",
       "                                         unmentioned  \\\n",
       "0                                                      \n",
       "1    \\nALL IN COLLUSION TOGETHER \\n\\n#NOJUSTICE \\...   \n",
       "2                                                      \n",
       "3   he will do in one year all the things you sho...   \n",
       "4  #CNN #newday clear #Trump deliberately throwin...   \n",
       "\n",
       "                                      depunctualized  \\\n",
       "0                                                      \n",
       "1    \\nALL IN COLLUSION TOGETHER \\n\\nNOJUSTICE \\n...   \n",
       "2                                                      \n",
       "3   he will do in one year all the things you sho...   \n",
       "4  CNN newday clear Trump deliberately throwing t...   \n",
       "\n",
       "                                              lemmed  \n",
       "0                                                 []  \n",
       "1                      [collus, nojustic, trumppenc]  \n",
       "2                                                 []  \n",
       "3                                      [year, thing]  \n",
       "4  [newday, clear, trump, deliber, throw, racein,...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('data/geotagged_tweets_20160812-0912.jsons', 'r', encoding='utf-8') as f:\n",
    "    all_tweets = [json.loads(line)['text'] for line in f]\n",
    "            \n",
    "            \n",
    "\n",
    "#clean the subset of tweets\n",
    "allframe = pd.DataFrame(all_tweets, columns=['text'])\n",
    "allframe['no_urls'] = allframe['text'].apply(lambda x: remove_urls(x)) #remove urls\n",
    "allframe['unmentioned'] = allframe['no_urls'].apply(lambda x: remove_mentions(x)) #remove mentions\n",
    "allframe['depunctualized'] = allframe['unmentioned'].apply(lambda x: remove_punct(x)) #remove punctuations\n",
    "allframe['lemmed'] = allframe['depunctualized'].apply(lambda x: preprocess(x)) #lemmatize\n",
    "allframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 come\n",
      "1 continu\n",
      "2 mouth\n",
      "3 nevertrump\n",
      "4 recogn\n",
      "5 wouldnt\n",
      "6 comparison\n",
      "7 graph\n",
      "8 iraq\n",
      "9 maim\n",
      "10 post\n"
     ]
    }
   ],
   "source": [
    "dictionary = gensim.corpora.Dictionary(trump_negative['lemmed'])\n",
    "count = 0\n",
    "for k, v in dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(20, 1), (124, 1), (271, 2), (327, 1), (1146, 1), (1420, 1)]"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in trump_negative['lemmed']]\n",
    "bow_corpus[4310]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 20 (\"trump\") appears 1 time.\n",
      "Word 124 (\"go\") appears 1 time.\n",
      "Word 271 (\"wont\") appears 2 time.\n",
      "Word 327 (\"lose\") appears 1 time.\n",
      "Word 1146 (\"differ\") appears 1 time.\n",
      "Word 1420 (\"ryan\") appears 1 time.\n"
     ]
    }
   ],
   "source": [
    "bow_doc_4310 = bow_corpus[4310]\n",
    "for i in range(len(bow_doc_4310)):\n",
    "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_4310[i][0], \n",
    "                                               dictionary[bow_doc_4310[i][0]], \n",
    "bow_doc_4310[i][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=10, id2word=dictionary, passes=2, workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.044*\"tax\" + 0.042*\"releas\" + 0.038*\"return\" + 0.032*\"lie\" + 0.025*\"trump\" + 0.025*\"hide\" + 0.023*\"speech\" + 0.017*\"medic\" + 0.015*\"record\" + 0.014*\"putin\"\n",
      "Topic: 1 \n",
      "Words: 0.057*\"hillari\" + 0.054*\"trump\" + 0.036*\"clinton\" + 0.019*\"crook\" + 0.014*\"corrupt\" + 0.014*\"money\" + 0.013*\"obama\" + 0.013*\"email\" + 0.012*\"plan\" + 0.012*\"right\"\n",
      "Topic: 2 \n",
      "Words: 0.036*\"fuckyoudonald\" + 0.029*\"racist\" + 0.027*\"idiot\" + 0.024*\"didnt\" + 0.024*\"fuck\" + 0.024*\"shit\" + 0.017*\"ignor\" + 0.015*\"america\" + 0.015*\"disgust\" + 0.011*\"hell\"\n",
      "Topic: 3 \n",
      "Words: 0.077*\"like\" + 0.032*\"look\" + 0.025*\"deplor\" + 0.020*\"sick\" + 0.019*\"hillari\" + 0.015*\"need\" + 0.012*\"lose\" + 0.011*\"person\" + 0.010*\"debat\" + 0.010*\"call\"\n",
      "Topic: 4 \n",
      "Words: 0.030*\"news\" + 0.018*\"shame\" + 0.014*\"report\" + 0.014*\"loser\" + 0.013*\"hat\" + 0.012*\"live\" + 0.012*\"campaign\" + 0.011*\"probabl\" + 0.011*\"support\" + 0.010*\"school\"\n",
      "Topic: 5 \n",
      "Words: 0.034*\"liar\" + 0.025*\"tell\" + 0.022*\"believ\" + 0.020*\"wrong\" + 0.019*\"know\" + 0.019*\"away\" + 0.019*\"lie\" + 0.019*\"your\" + 0.019*\"go\" + 0.018*\"health\"\n",
      "Topic: 6 \n",
      "Words: 0.024*\"trump\" + 0.022*\"mexico\" + 0.020*\"break\" + 0.020*\"help\" + 0.020*\"kill\" + 0.018*\"trumppenc\" + 0.014*\"fact\" + 0.012*\"maga\" + 0.012*\"muslim\" + 0.012*\"need\"\n",
      "Topic: 7 \n",
      "Words: 0.036*\"come\" + 0.033*\"say\" + 0.029*\"hate\" + 0.023*\"wall\" + 0.016*\"build\" + 0.014*\"pay\" + 0.014*\"america\" + 0.013*\"mouth\" + 0.013*\"watch\" + 0.011*\"want\"\n",
      "Topic: 8 \n",
      "Words: 0.030*\"american\" + 0.024*\"nevertrump\" + 0.018*\"wait\" + 0.017*\"immigr\" + 0.016*\"answer\" + 0.015*\"question\" + 0.014*\"time\" + 0.013*\"polici\" + 0.012*\"lose\" + 0.011*\"job\"\n",
      "Topic: 9 \n",
      "Words: 0.093*\"dont\" + 0.045*\"think\" + 0.037*\"know\" + 0.033*\"want\" + 0.030*\"peopl\" + 0.027*\"vote\" + 0.020*\"black\" + 0.017*\"doesnt\" + 0.016*\"trump\" + 0.016*\"white\"\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "# tfidf = models.TfidfModel(bow_corpus)\n",
    "# corpus_tfidf = tfidf[bow_corpus]\n",
    "\n",
    "# for doc in corpus_tfidf:\n",
    "#     pprint(doc)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.080*\"today\" + 0.069*\"trump\" + 0.067*\"love\" + 0.043*\"help\" + 0.040*\"pressur\" + 0.040*\"fine\" + 0.038*\"rain\" + 0.037*\"forecast\" + 0.037*\"tempcrab\" + 0.037*\"orchard\"\n",
      "Topic: 1 \n",
      "Words: 0.053*\"maga\" + 0.043*\"trump\" + 0.039*\"hillaryclinton\" + 0.029*\"trumppenc\" + 0.027*\"truth\" + 0.024*\"donaldtrump\" + 0.021*\"tell\" + 0.019*\"hillari\" + 0.018*\"elect\" + 0.016*\"poll\"\n",
      "Topic: 2 \n",
      "Words: 0.019*\"question\" + 0.019*\"trump\" + 0.018*\"hillari\" + 0.018*\"obama\" + 0.015*\"wrong\" + 0.014*\"news\" + 0.013*\"live\" + 0.013*\"answer\" + 0.013*\"leader\" + 0.012*\"press\"\n",
      "Topic: 3 \n",
      "Words: 0.049*\"neverhillari\" + 0.040*\"crookedhillari\" + 0.029*\"health\" + 0.027*\"hillaryshealth\" + 0.021*\"hillari\" + 0.021*\"trump\" + 0.017*\"maga\" + 0.015*\"hillaryclinton\" + 0.015*\"women\" + 0.014*\"apolog\"\n",
      "Topic: 4 \n",
      "Words: 0.083*\"trump\" + 0.029*\"putin\" + 0.021*\"donald\" + 0.018*\"say\" + 0.017*\"presid\" + 0.015*\"agre\" + 0.013*\"plan\" + 0.013*\"campaign\" + 0.012*\"russia\" + 0.011*\"go\"\n",
      "Topic: 5 \n",
      "Words: 0.056*\"clinton\" + 0.054*\"hillari\" + 0.051*\"lie\" + 0.023*\"liar\" + 0.020*\"right\" + 0.019*\"trump\" + 0.019*\"email\" + 0.018*\"corrupt\" + 0.014*\"money\" + 0.014*\"releas\"\n",
      "Topic: 6 \n",
      "Words: 0.035*\"racist\" + 0.034*\"like\" + 0.033*\"great\" + 0.031*\"look\" + 0.029*\"your\" + 0.029*\"go\" + 0.027*\"america\" + 0.027*\"trump\" + 0.019*\"basket\" + 0.018*\"make\"\n",
      "Topic: 7 \n",
      "Words: 0.049*\"dont\" + 0.034*\"know\" + 0.030*\"think\" + 0.028*\"trump\" + 0.021*\"hillari\" + 0.020*\"want\" + 0.019*\"peopl\" + 0.018*\"need\" + 0.017*\"doesnt\" + 0.013*\"care\"\n",
      "Topic: 8 \n",
      "Words: 0.027*\"that\" + 0.027*\"good\" + 0.021*\"pneumonia\" + 0.019*\"tweet\" + 0.016*\"thing\" + 0.016*\"trump\" + 0.016*\"say\" + 0.015*\"white\" + 0.013*\"like\" + 0.013*\"time\"\n",
      "Topic: 9 \n",
      "Words: 0.067*\"vote\" + 0.058*\"deplor\" + 0.048*\"trump\" + 0.043*\"nevertrump\" + 0.043*\"imwithh\" + 0.042*\"american\" + 0.021*\"support\" + 0.016*\"hillari\" + 0.012*\"republican\" + 0.010*\"peopl\"\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Probably not scientifically correct\n",
    "\n",
    "# lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=10, id2word=dictionary, passes=2, workers=4)\n",
    "# for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "#     print('Topic: {} Word: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTopicForQuery(tweet):\n",
    "    ques_vec = []\n",
    "    ques_vec = dictionary.doc2bow(tweet)\n",
    "\n",
    "    topic_vec = []\n",
    "    topic_vec = lda_model[ques_vec]\n",
    "\n",
    "    word_count_array = np.empty((len(topic_vec), 2), dtype = np.object)\n",
    "    for i in range(len(topic_vec)):\n",
    "        word_count_array[i, 0] = topic_vec[i][0]\n",
    "        word_count_array[i, 1] = topic_vec[i][1]\n",
    "\n",
    "    idx = np.argsort(word_count_array[:, 1])\n",
    "    idx = idx[::-1]\n",
    "    word_count_array = word_count_array[idx]\n",
    "\n",
    "    final = []\n",
    "    final = lda_model.print_topic(word_count_array[0, 0], 1)\n",
    "\n",
    "    question_topic = final.split('*') ## as format is like \"probability * topic\"\n",
    "\n",
    "    return question_topic[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(53262, 11) (49440, 11) (133541, 11) (139651, 11)\n"
     ]
    }
   ],
   "source": [
    "clinton_negative = clinton.loc[clinton.sentiment=='Negative'].copy()\n",
    "clinton_positive = clinton.loc[clinton.sentiment=='Positive'].copy()\n",
    "\n",
    "trump_negative = trump.loc[trump.sentiment=='Negative'].copy()\n",
    "trump_positive = trump.loc[trump.sentiment=='Positive'].copy()\n",
    "\n",
    "print(clinton_negative.shape, clinton_positive.shape, trump_negative.shape, trump_positive.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.518395234758234 0.48862779822757574\n"
     ]
    }
   ],
   "source": [
    "print(clinton_negative.shape[0]/clinton_sentiment.shape[0], trump_negative.shape[0]/trump_sentiment.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "clinton_negative['lemmed'] = clinton_negative['tweet'].apply(lambda x: preprocess(x)) #lemmatize\n",
    "clinton_positive['lemmed'] = clinton_positive['tweet'].apply(lambda x: preprocess(x)) #lemmatize\n",
    "\n",
    "trump_negative['lemmed'] = trump_negative['tweet'].apply(lambda x: preprocess(x)) #lemmatize\n",
    "trump_positive['lemmed'] = trump_positive['tweet'].apply(lambda x: preprocess(x)) #lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "clinton_negative.to_pickle('data/clinton_negative.pkl')\n",
    "clinton_positive.to_pickle('data/clinton_positive.pkl')\n",
    "\n",
    "trump_negative.to_pickle('data/trump_negative.pkl')\n",
    "trump_positive.to_pickle('data/trump_positive.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['corrupt', 'perjur', 'treason', 'antun', 'soldout', 'america']"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clinton_negative['lemmed'][764042004137447425]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"racist\"'"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getTopicForQuery(clinton_negative['lemmed'][764042004137447425])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
