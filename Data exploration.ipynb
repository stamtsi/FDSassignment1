{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "import json\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import metrics\n",
    "\n",
    "stopword = nltk.corpus.stopwords.words('english')\n",
    "ps = nltk.PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if heavier emoji stripper is needed\n",
    "\n",
    "# def remove_emoji(string):\n",
    "#     emoji_pattern = re.compile(\"[\"\n",
    "#                                u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "#                                u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "#                                u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "#                                u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "#                                u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "#                                u\"\\U00002702-\\U000027B0\"\n",
    "#                                u\"\\U00002702-\\U000027B0\"\n",
    "#                                u\"\\U000024C2-\\U0001F251\"\n",
    "#                                u\"\\U0001f926-\\U0001f937\"\n",
    "#                                u\"\\U00010000-\\U0010ffff\"\n",
    "#                                u\"\\u2640-\\u2642\"\n",
    "#                                u\"\\u2600-\\u2B55\"\n",
    "#                                u\"\\u200d\"\n",
    "#                                u\"\\u23cf\"\n",
    "#                                u\"\\u23e9\"\n",
    "#                                u\"\\u231a\"\n",
    "#                                u\"\\ufe0f\"  # dingbats\n",
    "#                                u\"\\u3030\"\n",
    "#                                \"]+\", flags=re.UNICODE)\n",
    "#     return emoji_pattern.sub(r'', string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punct(text):\n",
    "    text  = \"\".join([char for char in text if char not in string.punctuation])\n",
    "    text = re.sub('[0-9]+', '', text)\n",
    "    return text\n",
    "\n",
    "def tokenization(text):\n",
    "    text = re.split('\\W+', text)\n",
    "    return text\n",
    "\n",
    "def remove_urls(text):\n",
    "    text = re.sub(r'https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()@:%_\\+.~#?&//=]*)', '', text)\n",
    "    return text\n",
    "\n",
    "def remove_mentions(text):\n",
    "    text = re.sub(r'(^|[^@\\w])@(\\w{1,15})\\b', '', text)\n",
    "    return text\n",
    "\n",
    "def remove_empty_tokens(text):\n",
    "    text = list(filter(None, text))\n",
    "    return text\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    text = [word for word in text if word not in stopword]\n",
    "    return text\n",
    "\n",
    "def stemming(text):\n",
    "    text = [ps.stem(word) for word in text]\n",
    "    return text\n",
    "\n",
    "def deEmojify(text):\n",
    "    regrex_pattern = re.compile(pattern = \"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           \"]+\", flags = re.UNICODE)\n",
    "    return regrex_pattern.sub(r'',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target          id                          date      flag  \\\n",
       "0       0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
       "1       0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "2       0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "3       0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4       0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "\n",
       "              user                                               text  \n",
       "0  _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
       "1    scotthamilton  is upset that he can't update his Facebook by ...  \n",
       "2         mattycus  @Kenichan I dived many times for the ball. Man...  \n",
       "3          ElleCTF    my whole body feels itchy and like its on fire   \n",
       "4           Karoli  @nationwideclass no, it's not behaving at all....  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 0 = negative, 4 = positive\n",
    "\n",
    "df = pd.read_csv('training.1600000.processed.noemoticon.csv', encoding='ISO-8859-1', header=None, names=['target', 'id', 'date', 'flag', 'user', 'text'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "      <th>no_urls</th>\n",
       "      <th>unmentioned</th>\n",
       "      <th>depunctualized</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>improved_tokenized</th>\n",
       "      <th>nonstop</th>\n",
       "      <th>stemmed</th>\n",
       "      <th>rejoined</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "      <td>@switchfoot  - Awww, that's a bummer.  You sho...</td>\n",
       "      <td>- Awww, that's a bummer.  You shoulda got Da...</td>\n",
       "      <td>Awww thats a bummer  You shoulda got David ...</td>\n",
       "      <td>[, awww, thats, a, bummer, you, shoulda, got, ...</td>\n",
       "      <td>[awww, thats, a, bummer, you, shoulda, got, da...</td>\n",
       "      <td>[awww, thats, bummer, shoulda, got, david, car...</td>\n",
       "      <td>[awww, that, bummer, shoulda, got, david, carr...</td>\n",
       "      <td>awww that bummer shoulda got david carr third day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "      <td>is upset that he cant update his Facebook by t...</td>\n",
       "      <td>[is, upset, that, he, cant, update, his, faceb...</td>\n",
       "      <td>[is, upset, that, he, cant, update, his, faceb...</td>\n",
       "      <td>[upset, cant, update, facebook, texting, might...</td>\n",
       "      <td>[upset, cant, updat, facebook, text, might, cr...</td>\n",
       "      <td>upset cant updat facebook text might cri resul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "      <td>I dived many times for the ball. Managed to s...</td>\n",
       "      <td>I dived many times for the ball Managed to sa...</td>\n",
       "      <td>[, i, dived, many, times, for, the, ball, mana...</td>\n",
       "      <td>[i, dived, many, times, for, the, ball, manage...</td>\n",
       "      <td>[dived, many, times, ball, managed, save, rest...</td>\n",
       "      <td>[dive, mani, time, ball, manag, save, rest, go...</td>\n",
       "      <td>dive mani time ball manag save rest go bound</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>[my, whole, body, feels, itchy, and, like, its...</td>\n",
       "      <td>[my, whole, body, feels, itchy, and, like, its...</td>\n",
       "      <td>[whole, body, feels, itchy, like, fire]</td>\n",
       "      <td>[whole, bodi, feel, itchi, like, fire]</td>\n",
       "      <td>whole bodi feel itchi like fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "      <td>no, it's not behaving at all. i'm mad. why am...</td>\n",
       "      <td>no its not behaving at all im mad why am i he...</td>\n",
       "      <td>[, no, its, not, behaving, at, all, im, mad, w...</td>\n",
       "      <td>[no, its, not, behaving, at, all, im, mad, why...</td>\n",
       "      <td>[behaving, im, mad, cant, see]</td>\n",
       "      <td>[behav, im, mad, cant, see]</td>\n",
       "      <td>behav im mad cant see</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target          id                          date      flag  \\\n",
       "0       0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
       "1       0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "2       0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "3       0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4       0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "\n",
       "              user                                               text  \\\n",
       "0  _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...   \n",
       "1    scotthamilton  is upset that he can't update his Facebook by ...   \n",
       "2         mattycus  @Kenichan I dived many times for the ball. Man...   \n",
       "3          ElleCTF    my whole body feels itchy and like its on fire    \n",
       "4           Karoli  @nationwideclass no, it's not behaving at all....   \n",
       "\n",
       "                                             no_urls  \\\n",
       "0  @switchfoot  - Awww, that's a bummer.  You sho...   \n",
       "1  is upset that he can't update his Facebook by ...   \n",
       "2  @Kenichan I dived many times for the ball. Man...   \n",
       "3    my whole body feels itchy and like its on fire    \n",
       "4  @nationwideclass no, it's not behaving at all....   \n",
       "\n",
       "                                         unmentioned  \\\n",
       "0    - Awww, that's a bummer.  You shoulda got Da...   \n",
       "1  is upset that he can't update his Facebook by ...   \n",
       "2   I dived many times for the ball. Managed to s...   \n",
       "3    my whole body feels itchy and like its on fire    \n",
       "4   no, it's not behaving at all. i'm mad. why am...   \n",
       "\n",
       "                                      depunctualized  \\\n",
       "0     Awww thats a bummer  You shoulda got David ...   \n",
       "1  is upset that he cant update his Facebook by t...   \n",
       "2   I dived many times for the ball Managed to sa...   \n",
       "3    my whole body feels itchy and like its on fire    \n",
       "4   no its not behaving at all im mad why am i he...   \n",
       "\n",
       "                                           tokenized  \\\n",
       "0  [, awww, thats, a, bummer, you, shoulda, got, ...   \n",
       "1  [is, upset, that, he, cant, update, his, faceb...   \n",
       "2  [, i, dived, many, times, for, the, ball, mana...   \n",
       "3  [my, whole, body, feels, itchy, and, like, its...   \n",
       "4  [, no, its, not, behaving, at, all, im, mad, w...   \n",
       "\n",
       "                                  improved_tokenized  \\\n",
       "0  [awww, thats, a, bummer, you, shoulda, got, da...   \n",
       "1  [is, upset, that, he, cant, update, his, faceb...   \n",
       "2  [i, dived, many, times, for, the, ball, manage...   \n",
       "3  [my, whole, body, feels, itchy, and, like, its...   \n",
       "4  [no, its, not, behaving, at, all, im, mad, why...   \n",
       "\n",
       "                                             nonstop  \\\n",
       "0  [awww, thats, bummer, shoulda, got, david, car...   \n",
       "1  [upset, cant, update, facebook, texting, might...   \n",
       "2  [dived, many, times, ball, managed, save, rest...   \n",
       "3            [whole, body, feels, itchy, like, fire]   \n",
       "4                     [behaving, im, mad, cant, see]   \n",
       "\n",
       "                                             stemmed  \\\n",
       "0  [awww, that, bummer, shoulda, got, david, carr...   \n",
       "1  [upset, cant, updat, facebook, text, might, cr...   \n",
       "2  [dive, mani, time, ball, manag, save, rest, go...   \n",
       "3             [whole, bodi, feel, itchi, like, fire]   \n",
       "4                        [behav, im, mad, cant, see]   \n",
       "\n",
       "                                            rejoined  \n",
       "0  awww that bummer shoulda got david carr third day  \n",
       "1  upset cant updat facebook text might cri resul...  \n",
       "2       dive mani time ball manag save rest go bound  \n",
       "3                    whole bodi feel itchi like fire  \n",
       "4                              behav im mad cant see  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['no_urls'] = df['text'].apply(lambda x: remove_urls(x)) #remove urls\n",
    "df['unmentioned'] = df['no_urls'].apply(lambda x: remove_mentions(x)) #remove mentions\n",
    "df['depunctualized'] = df['unmentioned'].apply(lambda x: remove_punct(x)) #remove punctuations\n",
    "df['tokenized'] = df['depunctualized'].apply(lambda x: tokenization(x.lower())) #tokenize\n",
    "df['improved_tokenized'] = df['tokenized'].apply(lambda x: remove_empty_tokens(x)) #remove empty tokens\n",
    "df['nonstop'] = df['improved_tokenized'].apply(lambda x: remove_stopwords(x)) #remove stopwords\n",
    "df['stemmed'] = df['nonstop'].apply(lambda x: stemming(x)) #stem the tokens\n",
    "df['rejoined'] = df['stemmed'].apply(lambda x: \" \".join(x)) #rejoin for vectorization\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract to numpy arrays\n",
    "\n",
    "y = df.target.to_numpy()\n",
    "X = df.rejoined.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [ 676295 1080293   77706 ...    9768  481696 1177386] TEST: [ 475733  253127 1507988 ... 1139612 1488894 1556879]\n"
     ]
    }
   ],
   "source": [
    "#split into train (80%) and test (20%)\n",
    "\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, train_size=0.8)\n",
    "\n",
    "for train_index, test_index in sss.split(X, y):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_t = X[train_index], X[test_index]\n",
    "    y_train, y_t = y[train_index], y[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST: [ 74429  54573 254983 ... 252588  38747 171811] Val: [129815 234404 135075 ... 200157 157826  52091]\n"
     ]
    }
   ],
   "source": [
    "#split test into test (10%) and val (10%)\n",
    "\n",
    "sss_val = StratifiedShuffleSplit(n_splits=1, test_size=0.5, train_size=0.5)\n",
    "\n",
    "for test_i, val_i in sss_val.split(X_t, y_t):\n",
    "    print(\"TEST:\", test_i, \"Val:\", val_i)\n",
    "    X_test, X_val = X_t[test_i], X_t[val_i]\n",
    "    y_test, y_val = y_t[test_i], y_t[val_i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1280000,) (1280000,) (160000,) (160000,) (160000,) (160000,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape, X_val.shape, y_val.shape) #check if split sizes are correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vectorize the tweets\n",
    "\n",
    "cv = CountVectorizer(binary=True)\n",
    "cv.fit(X_train) #fit the vectorizer\n",
    "\n",
    "X_train_vectorized = cv.transform(X_train)\n",
    "X_test_vectorized = cv.transform(X_test)\n",
    "X_val_vectorized = cv.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 160000 points : 37082\n"
     ]
    }
   ],
   "source": [
    "y_pred = MultinomialNB().fit(X_train_vectorized, y_train).predict(X_val_vectorized)\n",
    "print(\"Number of mislabeled points out of a total %d points : %d\"\n",
    "       % (X_val_vectorized.shape[0], (y_val != y_pred).sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7682375\n"
     ]
    }
   ],
   "source": [
    "mnb = MultinomialNB().fit(X_train_vectorized, y_train) #final trained model\n",
    "mnb_predictions = mnb.predict(X_val_vectorized)\n",
    "mnb_accuracy = mnb.score(X_val_vectorized, y_val)\n",
    "print(mnb_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    precision    recall  f1-score   support\n",
      "\n",
      "negative sentiment      0.758     0.788     0.773     80000\n",
      "positive sentiment      0.780     0.748     0.763     80000\n",
      "\n",
      "          accuracy                          0.768    160000\n",
      "         macro avg      0.769     0.768     0.768    160000\n",
      "      weighted avg      0.769     0.768     0.768    160000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_val, mnb_predictions, digits=3, target_names=['negative sentiment', 'positive sentiment']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save trained model as pickle\n",
    "\n",
    "# filename = 'MNB_sentiment_classifier_model_lemmatized.sav'\n",
    "# with open(filename, 'wb') as f_out:\n",
    "#     pickle.dump((mnb, cv), f_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'score'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-77-60268d3bf57b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mloaded_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloaded_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'score'"
     ]
    }
   ],
   "source": [
    "#load the model from disk\n",
    "filename = 'MNB_sentiment_classifier_model_lemmatized.sav'\n",
    "\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "result = loaded_model.score(X_test, Y_test)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_cal_vectorized' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-78-6093a0bff532>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m#print(result)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mmnb_accuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmnb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_cal_vectorized\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmnb_accuracy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_cal_vectorized' is not defined"
     ]
    }
   ],
   "source": [
    "#load the model from disk\n",
    "filename = 'MNB_sentiment_classifier_model_lemmatized.sav'\n",
    "\n",
    "mnb, cv = pickle.load(open(filename, 'rb'))\n",
    "#result = loaded_model.score(X_test, Y_test)\n",
    "#print(result)\n",
    "\n",
    "mnb_accuracy = mnb.score(X_cal_vectorized, y_val)\n",
    "print(mnb_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the model on provided tweet dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#take a subset of 30 tweets\n",
    "\n",
    "jsonlist = []\n",
    "c = 0\n",
    "with open('geotagged_tweets_20160812-0912.jsons', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        c+=1\n",
    "        jsonlist.append(json.loads(line))\n",
    "        if c == 30:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    tweet_file = ['geotagged_tweets_20160812-0912.json']\n",
    "    tweets = []\n",
    "    for file in tweet_files:\n",
    "        with open(file, 'r')as f:\n",
    "            for line in f.readlines():\n",
    "                tweets.append(json.loads(line))\n",
    "    populate_tweet_df(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-56-c138d136b316>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-56-c138d136b316>\"\u001b[1;36m, line \u001b[1;32m4\u001b[0m\n\u001b[1;33m    df['created_at']= df['created_at'].apply(lamda x: tweet(x))\u001b[0m\n\u001b[1;37m                                                   ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def populate_tweet_df(tweets):\n",
    "    df=pd.DataFrame()\n",
    "    \n",
    "    df['created_at']= df['created_at'].apply(lamda x: tweet(x))\n",
    "    \n",
    "    print(df)\n",
    "    \n",
    " \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " body={\"source\": dict_data[\"source\"],\n",
    "                           \"created_at\": dict_data[\"user\"][\"created_at\"],\n",
    "                           \"author\": dict_data[\"user\"][\"screen_name\"],\n",
    "                           \"location\": dict_data[\"user\"][\"location\"],\n",
    "                           \"time_zone\": dict_data[\"user\"][\"time_zone\"],\n",
    "                           \"followers\": dict_data[\"user\"][\"followers_count\"],\n",
    "                           \"friends\": dict_data[\"user\"][\"friends_count\"],\n",
    "                           \"timestamp\": timestamp,\n",
    "                           \"message\": dict_data[\"text\"],\n",
    "                           \"polarity\": tweet.sentiment.polarity,\n",
    "                           \"subjectivity\": tweet.sentiment.subjectivity,\n",
    "                           \"sentiment\": sentiment})\n",
    "    \n",
    "    \n",
    "     dict_data = json.loads(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_input' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-38534deb5412>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#clean the subset of tweets\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mtestframe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mtestframe\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'no_urls'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtestframe\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mremove_urls\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#remove urls\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtestframe\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'unmentioned'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtestframe\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'no_urls'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mremove_mentions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#remove mentions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'test_input' is not defined"
     ]
    }
   ],
   "source": [
    "#clean the subset of tweets\n",
    "\n",
    "testframe = pd.DataFrame(test_input, columns=['text'])\n",
    "testframe['no_urls'] = testframe['text'].apply(lambda x: remove_urls(x)) #remove urls\n",
    "testframe['unmentioned'] = testframe['no_urls'].apply(lambda x: remove_mentions(x)) #remove mentions\n",
    "testframe['depunctualized'] = testframe['unmentioned'].apply(lambda x: remove_punct(x)) #remove punctuations\n",
    "testframe['tokenized'] = testframe['depunctualized'].apply(lambda x: tokenization(x.lower())) #tokenize\n",
    "testframe['improved_tokenized'] = testframe['tokenized'].apply(lambda x: remove_empty_tokens(x)) #remove empty tokens\n",
    "testframe['nonstop'] = testframe['improved_tokenized'].apply(lambda x: remove_stopwords(x)) #remove stopwords\n",
    "testframe['stemmed'] = testframe['nonstop'].apply(lambda x: stemming(x))\n",
    "testframe['rejoined'] = testframe['stemmed'].apply(lambda x: \" \".join(x))\n",
    "testframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'testframe' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-e417fde987b1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtestinput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtweet\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtweet\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtestframe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrejoined\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mtweet\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m#drop empty tweets\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtestinput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'testframe' is not defined"
     ]
    }
   ],
   "source": [
    "testinput = [tweet for tweet in testframe.rejoined.values if tweet] #drop empty tweets\n",
    "testinput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'testinput' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-54be5013e22f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#vectorize with the fitted vectorizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mtest_input_vectorized\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtestinput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'testinput' is not defined"
     ]
    }
   ],
   "source": [
    "#vectorize with the fitted vectorizer\n",
    "\n",
    "test_input_vectorized = cv.transform(testinput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_input_vectorized' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-3d3bbf23268f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmnb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_input_vectorized\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#predict the sentiment of the tweets\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'test_input_vectorized' is not defined"
     ]
    }
   ],
   "source": [
    "test = mnb.predict(test_input_vectorized) #predict the sentiment of the tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentiment and stemmed tweet\n",
    "\n",
    "output = pd.DataFrame(testinput, test).reset_index()\n",
    "output.columns = ['sentiment', 'tweet']\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pre cleaned tweet\n",
    "\n",
    "for i in testframe.text:\n",
    "    print(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
