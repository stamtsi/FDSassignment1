{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Negative words model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "import json\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import metrics\n",
    "\n",
    "stopword = nltk.corpus.stopwords.words('english')\n",
    "ps = nltk.PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup client\n",
    "client = MongoClient(\"localhost\", 27017)\n",
    "db = client.TwitterData\n",
    "collection = db.Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punct(text):\n",
    "    text  = \"\".join([char for char in text if char not in string.punctuation])\n",
    "    text = re.sub('[0-9]+', '', text)\n",
    "    return text\n",
    "\n",
    "def tokenization(text):\n",
    "    text = re.split('\\W+', text)\n",
    "    return text\n",
    "\n",
    "def remove_urls(text):\n",
    "    text = re.sub(r'https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()@:%_\\+.~#?&//=]*)', '', text)\n",
    "    return text\n",
    "\n",
    "def remove_mentions(text):\n",
    "    text = re.sub(r'(^|[^@\\w])@(\\w{1,15})\\b', '', text)\n",
    "    return text\n",
    "\n",
    "def remove_hashtags(text):\n",
    "    text = re.sub(r'(^|[^#\\w])#(\\w{1,50})\\b', '', text);\n",
    "    return text\n",
    "\n",
    "def remove_empty_tokens(text):\n",
    "    text = list(filter(None, text))\n",
    "    return text\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    text = [word for word in text if word not in stopword]\n",
    "    return text\n",
    "\n",
    "def stemming(text):\n",
    "    text = [ps.stem(word) for word in text]\n",
    "    return text\n",
    "\n",
    "def lemmatize(text):\n",
    "    lem = WordNetLemmatizer()\n",
    "    text = [lem.lemmatize(word, pos=\"v\") for word in text]\n",
    "    return text\n",
    "\n",
    "def deEmojify(text):\n",
    "    regrex_pattern = re.compile(pattern = \"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           \"]+\", flags = re.UNICODE)\n",
    "    return regrex_pattern.sub(r'',text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "657307"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = {}\n",
    "projection = {\"text\": 1, \"_id\": 0}\n",
    "\n",
    "# Get data from Mongo to Pandas\n",
    "cursor = collection.find(query, projection)\n",
    "df =  pd.DataFrame(list(cursor))\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_subset = df.sample(n=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>no_urls</th>\n",
       "      <th>unmentioned</th>\n",
       "      <th>unhashtagged</th>\n",
       "      <th>depunctualized</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>improved_tokenized</th>\n",
       "      <th>nonstop</th>\n",
       "      <th>lemmatized</th>\n",
       "      <th>rejoined</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>49193</th>\n",
       "      <td>@FoxNews @billclinton @HillaryClinton The Clin...</td>\n",
       "      <td>@FoxNews @billclinton @HillaryClinton The Clin...</td>\n",
       "      <td>The Clintons already have Trumps tax return v...</td>\n",
       "      <td>The Clintons already have Trumps tax return v...</td>\n",
       "      <td>The Clintons already have Trumps tax return v...</td>\n",
       "      <td>[, the, clintons, already, have, trumps, tax, ...</td>\n",
       "      <td>[the, clintons, already, have, trumps, tax, re...</td>\n",
       "      <td>[clintons, already, trumps, tax, return, via, ...</td>\n",
       "      <td>[clintons, already, trump, tax, return, via, c...</td>\n",
       "      <td>clintons already trump tax return via corrupt irs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130662</th>\n",
       "      <td>So he'll leave after #HillaryClinton wins in N...</td>\n",
       "      <td>So he'll leave after #HillaryClinton wins in N...</td>\n",
       "      <td>So he'll leave after #HillaryClinton wins in N...</td>\n",
       "      <td>So he'll leave after wins in November?</td>\n",
       "      <td>So hell leave after wins in November</td>\n",
       "      <td>[so, hell, leave, after, wins, in, november, ]</td>\n",
       "      <td>[so, hell, leave, after, wins, in, november]</td>\n",
       "      <td>[hell, leave, wins, november]</td>\n",
       "      <td>[hell, leave, win, november]</td>\n",
       "      <td>hell leave win november</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>595232</th>\n",
       "      <td>@HalleyBorderCol @smp0711 @FoxBusiness @LouDob...</td>\n",
       "      <td>@HalleyBorderCol @smp0711 @FoxBusiness @LouDob...</td>\n",
       "      <td>Because a Communist supports Hillary does tha...</td>\n",
       "      <td>Because a Communist supports Hillary does tha...</td>\n",
       "      <td>Because a Communist supports Hillary does tha...</td>\n",
       "      <td>[, because, a, communist, supports, hillary, d...</td>\n",
       "      <td>[because, a, communist, supports, hillary, doe...</td>\n",
       "      <td>[communist, supports, hillary, make, communist]</td>\n",
       "      <td>[communist, support, hillary, make, communist]</td>\n",
       "      <td>communist support hillary make communist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417267</th>\n",
       "      <td>@HillaryClinton tried to donate many times ove...</td>\n",
       "      <td>@HillaryClinton tried to donate many times ove...</td>\n",
       "      <td>tried to donate many times over the last 2 m ...</td>\n",
       "      <td>tried to donate many times over the last 2 m ...</td>\n",
       "      <td>tried to donate many times over the last  m l...</td>\n",
       "      <td>[, tried, to, donate, many, times, over, the, ...</td>\n",
       "      <td>[tried, to, donate, many, times, over, the, la...</td>\n",
       "      <td>[tried, donate, many, times, last, link, n, em...</td>\n",
       "      <td>[try, donate, many, time, last, link, n, email...</td>\n",
       "      <td>try donate many time last link n email frm rob...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>611006</th>\n",
       "      <td>@MikePenceVP You just left out #DavidDuke,#Ste...</td>\n",
       "      <td>@MikePenceVP You just left out #DavidDuke,#Ste...</td>\n",
       "      <td>You just left out #DavidDuke,#SteveBannon,#Al...</td>\n",
       "      <td>You just left out &amp;amp; in your speech !</td>\n",
       "      <td>You just left out amp in your speech</td>\n",
       "      <td>[, you, just, left, out, amp, in, your, speech, ]</td>\n",
       "      <td>[you, just, left, out, amp, in, your, speech]</td>\n",
       "      <td>[left, amp, speech]</td>\n",
       "      <td>[leave, amp, speech]</td>\n",
       "      <td>leave amp speech</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  \\\n",
       "49193   @FoxNews @billclinton @HillaryClinton The Clin...   \n",
       "130662  So he'll leave after #HillaryClinton wins in N...   \n",
       "595232  @HalleyBorderCol @smp0711 @FoxBusiness @LouDob...   \n",
       "417267  @HillaryClinton tried to donate many times ove...   \n",
       "611006  @MikePenceVP You just left out #DavidDuke,#Ste...   \n",
       "\n",
       "                                                  no_urls  \\\n",
       "49193   @FoxNews @billclinton @HillaryClinton The Clin...   \n",
       "130662  So he'll leave after #HillaryClinton wins in N...   \n",
       "595232  @HalleyBorderCol @smp0711 @FoxBusiness @LouDob...   \n",
       "417267  @HillaryClinton tried to donate many times ove...   \n",
       "611006  @MikePenceVP You just left out #DavidDuke,#Ste...   \n",
       "\n",
       "                                              unmentioned  \\\n",
       "49193    The Clintons already have Trumps tax return v...   \n",
       "130662  So he'll leave after #HillaryClinton wins in N...   \n",
       "595232   Because a Communist supports Hillary does tha...   \n",
       "417267   tried to donate many times over the last 2 m ...   \n",
       "611006   You just left out #DavidDuke,#SteveBannon,#Al...   \n",
       "\n",
       "                                             unhashtagged  \\\n",
       "49193    The Clintons already have Trumps tax return v...   \n",
       "130662           So he'll leave after wins in November?     \n",
       "595232   Because a Communist supports Hillary does tha...   \n",
       "417267   tried to donate many times over the last 2 m ...   \n",
       "611006           You just left out &amp; in your speech !   \n",
       "\n",
       "                                           depunctualized  \\\n",
       "49193    The Clintons already have Trumps tax return v...   \n",
       "130662             So hell leave after wins in November     \n",
       "595232   Because a Communist supports Hillary does tha...   \n",
       "417267   tried to donate many times over the last  m l...   \n",
       "611006              You just left out amp in your speech    \n",
       "\n",
       "                                                tokenized  \\\n",
       "49193   [, the, clintons, already, have, trumps, tax, ...   \n",
       "130662     [so, hell, leave, after, wins, in, november, ]   \n",
       "595232  [, because, a, communist, supports, hillary, d...   \n",
       "417267  [, tried, to, donate, many, times, over, the, ...   \n",
       "611006  [, you, just, left, out, amp, in, your, speech, ]   \n",
       "\n",
       "                                       improved_tokenized  \\\n",
       "49193   [the, clintons, already, have, trumps, tax, re...   \n",
       "130662       [so, hell, leave, after, wins, in, november]   \n",
       "595232  [because, a, communist, supports, hillary, doe...   \n",
       "417267  [tried, to, donate, many, times, over, the, la...   \n",
       "611006      [you, just, left, out, amp, in, your, speech]   \n",
       "\n",
       "                                                  nonstop  \\\n",
       "49193   [clintons, already, trumps, tax, return, via, ...   \n",
       "130662                      [hell, leave, wins, november]   \n",
       "595232    [communist, supports, hillary, make, communist]   \n",
       "417267  [tried, donate, many, times, last, link, n, em...   \n",
       "611006                                [left, amp, speech]   \n",
       "\n",
       "                                               lemmatized  \\\n",
       "49193   [clintons, already, trump, tax, return, via, c...   \n",
       "130662                       [hell, leave, win, november]   \n",
       "595232     [communist, support, hillary, make, communist]   \n",
       "417267  [try, donate, many, time, last, link, n, email...   \n",
       "611006                               [leave, amp, speech]   \n",
       "\n",
       "                                                 rejoined  \n",
       "49193   clintons already trump tax return via corrupt irs  \n",
       "130662                            hell leave win november  \n",
       "595232           communist support hillary make communist  \n",
       "417267  try donate many time last link n email frm rob...  \n",
       "611006                                   leave amp speech  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set = train_subset.copy()\n",
    "train_set['no_urls'] = train_set['text'].apply(lambda x: remove_urls(x)) #remove urls\n",
    "train_set['unmentioned'] = train_set['no_urls'].apply(lambda x: remove_mentions(x)) #remove mentions\n",
    "train_set['unhashtagged'] = train_set['unmentioned'].apply(lambda x: remove_hashtags(x)) #remove mentions\n",
    "train_set['depunctualized'] = train_set['unhashtagged'].apply(lambda x: remove_punct(x)) #remove punctuations\n",
    "train_set['tokenized'] = train_set['depunctualized'].apply(lambda x: tokenization(x.lower())) #tokenize\n",
    "train_set['improved_tokenized'] = train_set['tokenized'].apply(lambda x: remove_empty_tokens(x)) #remove empty tokens\n",
    "train_set['nonstop'] = train_set['improved_tokenized'].apply(lambda x: remove_stopwords(x)) #remove stopwords\n",
    "train_set['lemmatized'] = train_set['nonstop'].apply(lambda x: lemmatize(x)) #stem the tokens\n",
    "train_set['rejoined'] = train_set['lemmatized'].apply(lambda x: \" \".join(x)) #rejoin for vectorization\n",
    "train_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set['rejoined'].replace('', np.nan, inplace=True)\n",
    "train_set.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Adding negative words\n",
    "negative_words = pd.read_csv(\"data/negative_words.txt\", sep=\"\\n\", header=None, names=[\"word\"])\n",
    "negative_words = negative_words.word.values.tolist()\n",
    "\n",
    "def has_negative_word(text):\n",
    "    for word in text.split():\n",
    "        if word in negative_words:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set[\"has_negative_word\"] = train_set[\"rejoined\"].apply(lambda x: has_negative_word(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-10-bf110fd8e32e>:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_set_input[\"target\"] = train_set_input.target.apply(lambda x: 0 if x is True else 4)\n"
     ]
    }
   ],
   "source": [
    "## Prepare input\n",
    "train_set_input = train_set.iloc[:, [-1, -2]]\n",
    "train_set_input.columns = [\"target\", \"rejoined\"]\n",
    "train_set_input[\"target\"] = train_set_input.target.apply(lambda x: 0 if x is True else 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train_set_input.target.to_numpy()\n",
    "X = train_set_input.rejoined.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [49692 53227 40459 ... 71448  8557 56422] TEST: [68896 82020 23674 ...  4796  1184  5547]\n"
     ]
    }
   ],
   "source": [
    "# Split into train (80%) and test (20%)\n",
    "\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, train_size=0.8)\n",
    "\n",
    "for train_index, test_index in sss.split(X, y):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_t = X[train_index], X[test_index]\n",
    "    y_train, y_t = y[train_index], y[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST: [ 9329 10580  7642 ... 12820 14986 17292] Val: [ 9766  2829 10343 ...  8624  5694  1646]\n"
     ]
    }
   ],
   "source": [
    "#split test into test (10%) and val (10%)\n",
    "\n",
    "sss_val = StratifiedShuffleSplit(n_splits=1, test_size=0.5, train_size=0.5)\n",
    "\n",
    "for test_i, val_i in sss_val.split(X_t, y_t):\n",
    "    print(\"TEST:\", test_i, \"Val:\", val_i)\n",
    "    X_test, X_val = X_t[test_i], X_t[val_i]\n",
    "    y_test, y_val = y_t[test_i], y_t[val_i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(71592,) (71592,) (8949,) (8949,) (8950,) (8950,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape, X_val.shape, y_val.shape) #check if split sizes are correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vectorize the tweets\n",
    "\n",
    "cv = CountVectorizer(binary=True, min_df=2)\n",
    "cv.fit(X_train) #fit the vectorizer\n",
    "\n",
    "X_train_vectorized = cv.transform(X_train)\n",
    "X_test_vectorized = cv.transform(X_test)\n",
    "X_val_vectorized = cv.transform(X_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 8950 points : 573\n"
     ]
    }
   ],
   "source": [
    "y_pred = MultinomialNB().fit(X_train_vectorized, y_train).predict(X_val_vectorized)\n",
    "print(\"Number of mislabeled points out of a total %d points : %d\"\n",
    "       % (X_val_vectorized.shape[0], (y_val != y_pred).sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9359776536312849\n"
     ]
    }
   ],
   "source": [
    "mnb = MultinomialNB().fit(X_train_vectorized, y_train) #final trained model\n",
    "mnb_predictions = mnb.predict(X_val_vectorized)\n",
    "mnb_accuracy = mnb.score(X_val_vectorized, y_val)\n",
    "print(mnb_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    precision    recall  f1-score   support\n",
      "\n",
      "negative sentiment      0.900     0.963     0.931      3985\n",
      "positive sentiment      0.968     0.914     0.941      4965\n",
      "\n",
      "          accuracy                          0.936      8950\n",
      "         macro avg      0.934     0.939     0.936      8950\n",
      "      weighted avg      0.938     0.936     0.936      8950\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_val, mnb_predictions, digits=3, target_names=['negative sentiment', 'positive sentiment']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save trained model as pickle\n",
    "\n",
    "#filename = 'neg_word_model.sav'\n",
    "#with open(filename, 'wb') as f_out:\n",
    "#    pickle.dump((mnb, cv), f_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing on Sentiment140 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target          id                          date      flag  \\\n",
       "0       0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
       "1       0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "2       0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "3       0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4       0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "\n",
       "              user                                               text  \n",
       "0  _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
       "1    scotthamilton  is upset that he can't update his Facebook by ...  \n",
       "2         mattycus  @Kenichan I dived many times for the ball. Man...  \n",
       "3          ElleCTF    my whole body feels itchy and like its on fire   \n",
       "4           Karoli  @nationwideclass no, it's not behaving at all....  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 0 = negative, 4 = positive\n",
    "\n",
    "df_testframe = pd.read_csv('data/training.1600000.processed.noemoticon.csv', encoding='ISO-8859-1', header=None, names=['target', 'id', 'date', 'flag', 'user', 'text'])\n",
    "df_testframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "      <th>no_urls</th>\n",
       "      <th>unmentioned</th>\n",
       "      <th>unhashtagged</th>\n",
       "      <th>depunctualized</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>improved_tokenized</th>\n",
       "      <th>nonstop</th>\n",
       "      <th>lemmatized</th>\n",
       "      <th>rejoined_lem</th>\n",
       "      <th>rejoined</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "      <td>@switchfoot  - Awww, that's a bummer.  You sho...</td>\n",
       "      <td>- Awww, that's a bummer.  You shoulda got Da...</td>\n",
       "      <td>- Awww, that's a bummer.  You shoulda got Da...</td>\n",
       "      <td>Awww thats a bummer  You shoulda got David ...</td>\n",
       "      <td>[, awww, thats, a, bummer, you, shoulda, got, ...</td>\n",
       "      <td>[awww, thats, a, bummer, you, shoulda, got, da...</td>\n",
       "      <td>[awww, thats, bummer, shoulda, got, david, car...</td>\n",
       "      <td>[awww, thats, bummer, shoulda, get, david, car...</td>\n",
       "      <td>awww thats bummer shoulda get david carr third...</td>\n",
       "      <td>awww thats bummer shoulda got david carr third...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "      <td>is upset that he cant update his Facebook by t...</td>\n",
       "      <td>[is, upset, that, he, cant, update, his, faceb...</td>\n",
       "      <td>[is, upset, that, he, cant, update, his, faceb...</td>\n",
       "      <td>[upset, cant, update, facebook, texting, might...</td>\n",
       "      <td>[upset, cant, update, facebook, texting, might...</td>\n",
       "      <td>upset cant update facebook texting might cry r...</td>\n",
       "      <td>upset cant update facebook texting might cry r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "      <td>I dived many times for the ball. Managed to s...</td>\n",
       "      <td>I dived many times for the ball. Managed to s...</td>\n",
       "      <td>I dived many times for the ball Managed to sa...</td>\n",
       "      <td>[, i, dived, many, times, for, the, ball, mana...</td>\n",
       "      <td>[i, dived, many, times, for, the, ball, manage...</td>\n",
       "      <td>[dived, many, times, ball, managed, save, rest...</td>\n",
       "      <td>[dive, many, time, ball, manage, save, rest, g...</td>\n",
       "      <td>dive many time ball manage save rest go bound</td>\n",
       "      <td>dived many times ball managed save rest go bounds</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>[my, whole, body, feels, itchy, and, like, its...</td>\n",
       "      <td>[my, whole, body, feels, itchy, and, like, its...</td>\n",
       "      <td>[whole, body, feels, itchy, like, fire]</td>\n",
       "      <td>[whole, body, feel, itchy, like, fire]</td>\n",
       "      <td>whole body feel itchy like fire</td>\n",
       "      <td>whole body feels itchy like fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "      <td>no, it's not behaving at all. i'm mad. why am...</td>\n",
       "      <td>no, it's not behaving at all. i'm mad. why am...</td>\n",
       "      <td>no its not behaving at all im mad why am i he...</td>\n",
       "      <td>[, no, its, not, behaving, at, all, im, mad, w...</td>\n",
       "      <td>[no, its, not, behaving, at, all, im, mad, why...</td>\n",
       "      <td>[behaving, im, mad, cant, see]</td>\n",
       "      <td>[behave, im, mad, cant, see]</td>\n",
       "      <td>behave im mad cant see</td>\n",
       "      <td>behaving im mad cant see</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target          id                          date      flag  \\\n",
       "0       0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
       "1       0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "2       0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "3       0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4       0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "\n",
       "              user                                               text  \\\n",
       "0  _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...   \n",
       "1    scotthamilton  is upset that he can't update his Facebook by ...   \n",
       "2         mattycus  @Kenichan I dived many times for the ball. Man...   \n",
       "3          ElleCTF    my whole body feels itchy and like its on fire    \n",
       "4           Karoli  @nationwideclass no, it's not behaving at all....   \n",
       "\n",
       "                                             no_urls  \\\n",
       "0  @switchfoot  - Awww, that's a bummer.  You sho...   \n",
       "1  is upset that he can't update his Facebook by ...   \n",
       "2  @Kenichan I dived many times for the ball. Man...   \n",
       "3    my whole body feels itchy and like its on fire    \n",
       "4  @nationwideclass no, it's not behaving at all....   \n",
       "\n",
       "                                         unmentioned  \\\n",
       "0    - Awww, that's a bummer.  You shoulda got Da...   \n",
       "1  is upset that he can't update his Facebook by ...   \n",
       "2   I dived many times for the ball. Managed to s...   \n",
       "3    my whole body feels itchy and like its on fire    \n",
       "4   no, it's not behaving at all. i'm mad. why am...   \n",
       "\n",
       "                                        unhashtagged  \\\n",
       "0    - Awww, that's a bummer.  You shoulda got Da...   \n",
       "1  is upset that he can't update his Facebook by ...   \n",
       "2   I dived many times for the ball. Managed to s...   \n",
       "3    my whole body feels itchy and like its on fire    \n",
       "4   no, it's not behaving at all. i'm mad. why am...   \n",
       "\n",
       "                                      depunctualized  \\\n",
       "0     Awww thats a bummer  You shoulda got David ...   \n",
       "1  is upset that he cant update his Facebook by t...   \n",
       "2   I dived many times for the ball Managed to sa...   \n",
       "3    my whole body feels itchy and like its on fire    \n",
       "4   no its not behaving at all im mad why am i he...   \n",
       "\n",
       "                                           tokenized  \\\n",
       "0  [, awww, thats, a, bummer, you, shoulda, got, ...   \n",
       "1  [is, upset, that, he, cant, update, his, faceb...   \n",
       "2  [, i, dived, many, times, for, the, ball, mana...   \n",
       "3  [my, whole, body, feels, itchy, and, like, its...   \n",
       "4  [, no, its, not, behaving, at, all, im, mad, w...   \n",
       "\n",
       "                                  improved_tokenized  \\\n",
       "0  [awww, thats, a, bummer, you, shoulda, got, da...   \n",
       "1  [is, upset, that, he, cant, update, his, faceb...   \n",
       "2  [i, dived, many, times, for, the, ball, manage...   \n",
       "3  [my, whole, body, feels, itchy, and, like, its...   \n",
       "4  [no, its, not, behaving, at, all, im, mad, why...   \n",
       "\n",
       "                                             nonstop  \\\n",
       "0  [awww, thats, bummer, shoulda, got, david, car...   \n",
       "1  [upset, cant, update, facebook, texting, might...   \n",
       "2  [dived, many, times, ball, managed, save, rest...   \n",
       "3            [whole, body, feels, itchy, like, fire]   \n",
       "4                     [behaving, im, mad, cant, see]   \n",
       "\n",
       "                                          lemmatized  \\\n",
       "0  [awww, thats, bummer, shoulda, get, david, car...   \n",
       "1  [upset, cant, update, facebook, texting, might...   \n",
       "2  [dive, many, time, ball, manage, save, rest, g...   \n",
       "3             [whole, body, feel, itchy, like, fire]   \n",
       "4                       [behave, im, mad, cant, see]   \n",
       "\n",
       "                                        rejoined_lem  \\\n",
       "0  awww thats bummer shoulda get david carr third...   \n",
       "1  upset cant update facebook texting might cry r...   \n",
       "2      dive many time ball manage save rest go bound   \n",
       "3                    whole body feel itchy like fire   \n",
       "4                             behave im mad cant see   \n",
       "\n",
       "                                            rejoined  \n",
       "0  awww thats bummer shoulda got david carr third...  \n",
       "1  upset cant update facebook texting might cry r...  \n",
       "2  dived many times ball managed save rest go bounds  \n",
       "3                   whole body feels itchy like fire  \n",
       "4                           behaving im mad cant see  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testframe = df_testframe\n",
    "\n",
    "testframe['no_urls'] = testframe['text'].apply(lambda x: remove_urls(x)) #remove urls\n",
    "testframe['unmentioned'] = testframe['no_urls'].apply(lambda x: remove_mentions(x)) #remove mentions\n",
    "testframe['unhashtagged'] = testframe['unmentioned'].apply(lambda x: remove_hashtags(x)) #remove mentions\n",
    "testframe['depunctualized'] = testframe['unhashtagged'].apply(lambda x: remove_punct(x)) #remove punctuations\n",
    "testframe['tokenized'] = testframe['depunctualized'].apply(lambda x: tokenization(x.lower())) #tokenize\n",
    "testframe['improved_tokenized'] = testframe['tokenized'].apply(lambda x: remove_empty_tokens(x)) #remove empty tokens\n",
    "testframe['nonstop'] = testframe['improved_tokenized'].apply(lambda x: remove_stopwords(x)) #remove stopwords\n",
    "testframe['lemmatized'] = testframe['nonstop'].apply(lambda x: lemmatize(x)) #stem the tokens\n",
    "testframe['rejoined_lem'] = testframe['lemmatized'].apply(lambda x: \" \".join(x)) #rejoin for vectorization\n",
    "testframe['rejoined'] = testframe['nonstop'].apply(lambda x: \" \".join(x)) #rejoin for vectorization\n",
    "testframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_frame = testframe[[\"target\", \"rejoined_lem\"]]\n",
    "testinput = [tweet[1] for tweet in t_frame.values if tweet[1]]\n",
    "testoutput = [tweet[0] for tweet in t_frame.values if tweet[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loading model\n",
    "model = pickle.load(open(\"test_lem.sav\", 'rb'))\n",
    "mnb = model[0]\n",
    "cv = model[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input_vectorized = cv.transform(testinput) #vectorize with the fitted vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = mnb.score(test_input_vectorized, testoutput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6484905376047017"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
